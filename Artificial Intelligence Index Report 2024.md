# Artificial Intelligence Index Report 2024

*Artificial Intelligence Index*
*Stanford Institute for Human-Centered Artificial Intelligence(HAI)*

---

## Top 10 Takeaways

1. AI beats humans on some tasks, but not on all. AI has surpassed human performance on several
benchmarks, including some in image classification, visual reasoning, and English understanding. Yet it trails
behind on more complex tasks like competition-level mathematics, visual commonsense reasoning and planning.
2. Industry continues to dominate frontier AI research. In 2023, industry produced 51 notable
machine learning models, while academia contributed only 15. There were also 21 notable models resulting from
industry-academia collaborations in 2023, a new high.
3. Frontier models get way more expensive. According to AI Index estimates, the training costs
of state-of-the-art AI models have reached unprecedented levels. For example, OpenAI’s GPT-4 used an
estimated $78 million worth of compute to train, while Google’s Gemini Ultra cost $191 million for compute.
4. The United States leads China, the EU, and the U.K. as the leading source of top AI
models. In 2023, 61 notable AI models originated from U.S.-based institutions, far outpacing the European
Union’s 21 and China’s 15.
5. Robust and standardized evaluations for LLM responsibility are seriously lacking.
New research from the AI Index reveals a significant lack of standardization in responsible AI reporting.
Leading developers, including OpenAI, Google, and Anthropic, primarily test their models against different
responsible AI benchmarks. This practice complicates efforts to systematically compare the risks and
limitations of top AI models.
6. Generative AI investment skyrockets. Despite a decline in overall AI private investment last
year, funding for generative AI surged, nearly octupling from 2022 to reach $25.2 billion. Major players in
the generative AI space, including OpenAI, Anthropic, Hugging Face, and Inflection, reported substantial
fundraising rounds.
7. The data is in: AI makes workers more productive and leads to higher quality work. In
2023, several studies assessed AI’s impact on labor, suggesting that AI enables workers to complete tasks more
quickly and to improve the quality of their output. These studies also demonstrated AI’s potential to bridge
the skill gap between low- and high-skilled workers. Still, other studies caution that using AI without proper
oversight can lead to diminished performance. 
8. Scientific progress accelerates even further, thanks to AI. In 2022, AI began to advance
scientific discovery. 2023, however, saw the launch of even more significant science-related AI applications—
from AlphaDev, which makes algorithmic sorting more efficient, to GNoME, which facilitates the process of
materials discovery.
9. The number of AI regulations in the United States sharply increases. The number of AIrelated regulations in the U.S. has risen significantly in the past year and over the last five years. In 2023, there
were 25 AI-related regulations, up from just one in 2016. Last year alone, the total number of AI-related regulations
grew by 56.3%.
10. People across the globe are more cognizant of AI’s potential impact—and more nervous.
A survey from Ipsos shows that, over the last year, the proportion of those who think AI will dramatically affect their
lives in the next three to five years has increased from 60% to 66%. Moreover, 52% express nervousness toward AI
products and services, marking a 13 percentage point rise from 2022. In America, Pew data suggests that 52% of
Americans report feeling more concerned than excited about AI, rising from 37% in 2022.

---

* [Chapter 1: Research and Development](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-1-research-and-development)

* [Chapter 2: Technical Performance](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-2-technical-performance)

* [Chapter 3: Responsible AI](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-3-responsible-ai)

* [Chapter 4: Economy](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-4-economy)

* [Chapter 5: Science and Medicine](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-5-science-and-medicine)

* [Chapter 6: Education](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-6-education)

* [Chapter 7: Policy and Governance](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-7-policy-and-governance)

* [Chapter 8: Diversity](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-8-diversity)

* [Chapter 9: Public Opinion](https://github.com/AlleninTaipei/Artificial-Intelligence-Introduction-for-Beginners/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md#chapter-9-public-opinion)

---

**Here are my summaries and notes.**

## Chapter 1: Research and Development

> This chapter studies trends in AI research and development. It begins by examining trends in AI publications and patents, and then examines trends in notable AI systems and foundation models. It concludes by analyzing AI conference attendance and open-sourceAI software projects.

||Chapter Highlights|
|-|-|
|1.|Industry continues to dominate frontier AI research. In 2023, industry produced 51 notable machine learning models, while academia contributed only 15. There were also 21 notable models resulting from industry-academia collaborations in 2023, a new high.|
|2.|More foundation models and more open foundation models. In 2023, a total of 149 foundation models were released, more than double the amount released in 2022. Of these newly released models, 65.7% were open-source, compared to only 44.4% in 2022 and 33.3% in 2021.|
|3.|Frontier models get way more expensive. According to AI Index estimates, the training costs of state-of-the-art AI models have reached unprecedented levels. For example, OpenAI’s GPT-4 used an estimated \$78 million worth of compute to train, while Google’s Gemini Ultra cost \$191 million for compute.
|4.|The United States leads China, the EU, and the U.K. as the leading source of top AI models. In 2023, 61 notable AI models originated from U.S.-based institutions, far outpacing the European Union’s 21 and China’s 15.|
|5.|The number of AI patents skyrockets. From 2021 to 2022, AI patent grants worldwide increased sharply by 62.7%. Since 2010, the number of granted AI patents has increased more than 31 times.|
|6.|China dominates AI patents. In 2022, China led global AI patent origins with 61.1%, significantly outpacing the United States, which accounted for 20.9% of AI patent origins. Since 2010, the U.S. share of AI patents has decreased from 54.1%.|
|7.|Open-source AI research explodes. Since 2011, the number of AI-related projects on GitHub has seen a consistent increase, growing from 845 in 2011 to approximately 1.8 million in 2023. Notably, there was a sharp 59.3% rise in the total number of GitHub AI projects in 2023 alone. The total number of stars for AI-related projects on GitHub also significantly increased in 2023, more than tripling from 4.0 million in 2022 to 12.2 million.|
|8.|The number of AI publications continues to rise. Between 2010 and 2022, the total number of AI publications nearly tripled, rising from approximately 88,000 in 2010 to more than 240,000 in 2022. The increase over the last year was a modest 1.1%.|

### 1.1 Publications

|Publications|Details|
|-|-|
|Total Number|The global count of AI publications nearly tripled between 2010 and 2022, rising from about 88,000 to over 240,000.|
||The increase in AI publications over the last year was modest, at 1.1%.|
|Type|In 2022, there were approximately 230,000 AI journal articles and roughly 42,000 conference submissions.|
||Since 2015, both AI journal and conference publications have been increasing at comparable rates.|
||In 2022, there were 2.6 times as many conference publications and 2.4 times as many journal publications compared to 2015.|
|Field|Machine learning publications have seen significant growth, increasing nearly sevenfold since 2015.|
||Other highly published AI fields in 2022 include computer vision, pattern recognition, and process management.|
|Sector|The academic sector contributed the majority (81.1%) of AI publications in 2022, maintaining its position as the leading global source of AI research over the past decade across all regions.|
||Industry participation is most significant in the United States, followed by the European Union plus the United Kingdom, and China.|
|AI Journal|AI journal publications experienced modest growth from 2010 to 2015 but grew approximately 2.4 times since 2015.|
||Between 2021 and 2022, there was a 4.5% increase in AI journal publications.|
|AI Conference|The number of AI conference publications has more than doubled since 2010.|
||Notably, there was a significant increase in AI conference publications in the past two years, with a 30.2% rise from 2021 to 2022, reaching 41,174 publications.|

### 1.2 Patents

>This section examines trends over time in global AI patents, which can reveal important insights into the evolution of innovation, research, and development within AI. Additionally, analyzing AI patents can reveal how these advancements are distributed globally. Similar to the publications data, there is a noticeable delay in AI patent data availability, with 2022 being the most recent year for which data is accessible. The data in this section comes from CSET.

|Patents|Details|
|-|-|
|Overall Growth in AI Patents|The number of AI patents granted globally has surged over the last decade, with a particularly sharp increase in recent years. Between 2010 and 2014, there was a 56.1% growth, but from 2021 to 2022 alone, there was a remarkable 62.7% increase.|
|Filing Status and Region|In 2022, the number of ungranted AI patents exceeded the number of granted ones by more than double. This represents a significant shift from previous years, where a larger proportion of filed AI patents were granted. This trend is consistent across major patent-originating regions like China, the European Union and United Kingdom, and the United States.|
|Regional Breakdown|As of 2022, the majority (75.2%) of granted AI patents originate from East Asia and the Pacific, with China being the largest contributor (61.1%). North America, led by the United States, follows at 21.2%. This marks a significant shift from previous years, where North America used to lead in global AI patents.|
|AI Patents Per Capita|South Korea leads in granted AI patents per capita, followed by Luxembourg and the United States. Singapore, South Korea, and China experienced substantial increases in AI patenting per capita from 2012 to 2022.|

### 1.3 Frontier AI Research

>This section explores the frontier of AI research. While many new AI models are introduced annually, only a small sample represents the most advanced research. Admittedly what constitutes advanced or frontier research is somewhat subjective. Frontier research could reflect a model posting a new state-of-the-art result on a benchmark, introducing a meaningful new architecture, or exercising some impressive new capabilities.

>The AI Index studies trends in two types of frontier AI models: “notable models” and foundation models.3 Epoch, an AI Index data provider, uses the term “notable machine learning models” to designate noteworthy models handpicked as being particularly influential within the AI/machine learning ecosystem. In contrast, foundation models are exceptionally large AI models trained on massive datasets, capable of performing a multitude of downstream tasks. Examples of foundation models include GPT-4, Claude 3, and Gemini. While many foundation models may qualify as notable models, not all notable models are foundation models.

>Within this section, the AI Index explores trends in notable models and foundation models from various perspectives, including originating organization, country of origin, parameter count, and compute usage. The analysis concludes with an examination of machine learning training costs.

* Epoch AI is a research organization dedicated to studying and predicting the evolution of advanced AI. They offer valuable insights into trends and developments in the field of machine learning.

|Trends|Details|
|-|-|
|Sector|Creating cutting-edge AI models now demands a substantial amount of data, computing power, and financial resources that are not available in academia. Since 2014, the industry has adopted take the lead.|
||In 2023, 51 notable models were produced by industry, 15 notable models emerged from academia and 21 notable models resulted from industry/academic partnerships, marking a new high.|
|National Affiliation|In 2023, the United States led with 61 notable machine learning models, followed by China with 15, and France with 8.|
||For the first time since 2019, the European Union and the United Kingdom together have surpassed China in the number of notable AI models produced.|
|Parameters|Parameter counts have sharply risen since the early 2010s.|
||This rise is attributed to increased data availability, hardware improvements, and the efficacy of larger models.|
||Numerical values learned during training that determine how a model interprets input data and makes predictions. Higher parameter counts often correlate with better performance.|
||High-parameter models are particularly notable in the industry sector, underscoring the capacity of companies like OpenAI, Anthropic, and Google to bear the computational costs of training on vast volumes of data.|
|Compute|More complex models and larger datasets require greater compute.|
||Higher compute demands lead to larger environmental footprints.|
||AlexNet (2012): Required an estimated 470 petaFLOPs.|
||Original Transformer (2017): Required around 7,400 petaFLOPs.|
||Google’s Gemini Ultra: One of the current state-of-the-art models, requiring 50 billion petaFLOPs.|
|Domain||
|Language Model|GPt-3 175B, Llama 2-70B|
|Vision Model|AlexNet|
|Multimodal|GPT-4|

* Introduced in 2023, the Ecosystem Graphs is a new community resource from Stanford that tracks the foundation model ecosystem, including datasets, models, and applications. This section uses data from
the Ecosystem Graphs to study trends in foundation models over time.

|Study|Details|
|-|-|
|Model Release Trends||
|No Access Models|These models, such as Google’s PaLM-E, are exclusively accessible to their developers.|
|Limited Access Models|Models like OpenAI’s GPT-4 fall into this category, offering restricted access typically through a public API.|
|Open Models|Examples include Meta’s Llama 2, where the model weights are fully released, allowing for modifications and free usage.|
|Growth in Number|Since 2019, the number of foundation models has seen exponential growth. In 2023, out of 149 foundation models released:|
||98 were open (65.8%)|
||23 were limited access (15.4%)|
||28 had no access (18.8%)|
|Organizational|Foundation models predominantly originate from industry sectors:
||In 2023, 72.5% of the models came from industry. Only 18.8% were from academia.|
||Google: 18 models|
||Meta: 11 models|
||Microsoft: 9 models|
||Among academic institutions, UC Berkeley led with three releases. Historically, since 2019, Google has released the most foundation models (40), followed by OpenAI (20). Tsinghua University and Stanford University are notable academic contributors, with seven and five releases, respectively.|
|National|Foundation models are also analyzed based on national affiliations, reflecting the geopolitical landscape of AI research:|
||In 2023, the majority of models originated from the United States (109), followed by China (20), and the United Kingdom. Cumulatively since 2019, the United States leads with 182 foundation models, followed by China (30) and the United Kingdom (21).|

|Training Cost|Details|
|-|-|
|Rising Costs|Training foundation models now costs millions of dollars and is increasing.|
|Example|Training GPT-4 cost over 100 million (mentioned by OpenAI's CEO, Sam Altman).|
|Impact on Academia|High costs have excluded universities from developing leading-edge foundation models.|
|Policy Initiatives|President Biden’s Executive Order on AI aims to create a National AI Research Resource to support non-industry actors.|
|AI Index Collaboration|AI Index and Epoch AI have collaborated to provide more robust estimates of AI training costs.|
|Methodology|Epoch AI analyzed training duration, type, quantity, and utilization rate of training hardware.|
|Cost Estimates Over Time|Transformer (2017)(900), GPT-4 (2023)(78 million), Google Gemini Ultra (2023)(191 million)|
|Significance|Model training costs have sharply increased over time. Illustrates the growing financial barriers to cutting-edge AI research.|

### 1.4 AI Conferences

>AI conferences serve as essential platforms for researchers to present their findings and network with peers and collaborators. Over the past two decades, these conferences have expanded in scale, quantity, and prestige. This section explores trends in attendance at major AI conferences. 

|Conferences|Details|
|-|-|
|Overall Trend|After a period of decline, likely due to the shift towards exclusively in-person formats, there has been an increase in conference attendance from 2022 to 2023. This reflects a 6.7% rise in total attendance over the last year. Since 2015, there has been a significant increase in annual attendees, indicating a growing interest in AI research and the emergence of new AI conferences.|
|Most Attended Conferences|Neural Information Processing Systems (NeurIPS) remains one of the most attended AI conferences, with approximately 16,380 participants in 2023. Other major AI conferences like ICML, ICCV, and AAAI also experienced year-over-year increases in attendance.|
|Variation in Attendance|While some conferences like NeurIPS, ICML, ICCV, and AAAI saw increases in attendance, others such as CVPR, ICRA, ICLR, and IROS observed slight declines in their attendance figures in the past year.|

### 1.5 Open-Source AI Software

>GitHub is a web-based platform that enables individuals and teams to host, review, and collaborate on code
repositories. Widely used by software developers, GitHub facilitates code management, project collaboration,
and open-source software support. This section draws on data from GitHub providing insights into broader trends in open-source AI software development not reflected in academic publication data.

|GitHub AI Projects|Details|
|-|-|
|Total Number|From 2011 to 2023, there has been a consistent increase in the number of AI-related GitHub projects, rising from 845 in 2011 to around 1.8 million in 2023. Notably, there was a significant 59.3% increase in the total number of GitHub AI projects in the last year alone.|
|Geographic Distribution|As of 2023, the United States had the highest share of GitHub AI projects at 22.9%, followed by India (19.0%), the European Union and the United Kingdom (17.9%). However, it's worth mentioning that the proportion of AI projects from developers located in the United States on GitHub has been declining steadily since 2016.|
|GitHub Stars|GitHub users can express their support for repositories by "starring" them, similar to liking a post on social media. Popular AI-related libraries such as TensorFlow, OpenCV, Keras, and PyTorch have received significant attention. The total number of stars for AI-related projects on GitHub more than tripled from 4.0 million in 2022 to 12.2 million in 2023, indicating a substantial increase in community support for open-source AI projects.|
|Geographic|In 2023, the United States received the highest number of GitHub stars, totaling 10.5 million. However, all major geographic regions observed, including the European Union and United Kingdom, China, and India, experienced a year-over-year increase in the total number of GitHub stars awarded to projects located in their countries.|
|Popular AI Libraries|TensorFlow: Library for building and deploying machine learning models.|
||OpenCV: Platform offering tools for computer vision, such as object detection and feature extraction.|
||Keras: User-friendly neural network library.|
||PyTorch: Framework for deep learning research and production.|

---

## Chapter 2: Technical Performance

>The technical performance section of this year’s AI Index offers a comprehensive overview of AI advancements in 2023. It starts with a high-level overview of AI technical performance, tracing its broad evolution over time. The chapter then examines the current state of a wide range of AI capabilities, including language processing, coding, computer vision (image and video analysis), reasoning, audio processing, autonomous agents, robotics, and reinforcement learning. It also shines a spotlight on notable AI research breakthroughs from the past year, exploring methods for improving Large Language Models through prompting, optimization, and fine-tuning, and wraps up with an exploration of AI systems’ environmental footprint.

||Chapter Highlights|
|-|-|
|1.|AI beats humans on some tasks, but not on all. AI has surpassed human performance on several benchmarks, including some in image classification, visual reasoning, and English understanding. Yet it trails behind on more complex tasks like competition-level mathematics, visual commonsense reasoning and planning.|
|2.|Here comes multimodal AI. Traditionally AI systems have been limited in scope, with language models excelling in text comprehension but faltering in image processing, and vice versa. However, recent advancements have led to the development of strong multimodal models, such as Google’s Gemini and OpenAI’s GPT-4. These models demonstrate flexibility and are capable of handling images and text and, in some instances, can even process audio.|
|3.|Harder benchmarks emerge. AI models have reached performance saturation on established benchmarks such as ImageNet, SQuAD, and SuperGLUE, prompting researchers to develop more challenging ones. In 2023, several challenging new benchmarks emerged, including SWE-bench for coding, HEIM for image generation, MMMU for general reasoning, MoCa for moral reasoning, AgentBench for agent-based behavior, and HaluEval for hallucinations.|
|4.|Better AI means better data which means … even better AI. New AI models such as SegmentAnything and Skoltech are being used to generate specialized data for tasks like image segmentation and 3D reconstruction. Data is vital for AI technical improvements. The use of AI to create more data enhances current capabilities and paves the way for future algorithmic improvements, especially on harder tasks.|
|5.|Human evaluation is in. With generative models producing high-quality text, images, and more, benchmarking has slowly started shifting toward incorporating human evaluations like the Chatbot Arena Leaderboard rather than computerized rankings like ImageNet or SQuAD. Public feeling about AI is becoming an increasingly important consideration in tracking AI progress.|
|6.|Thanks to Large Language Models, robots have become more flexible. The fusion of language modeling with robotics has given rise to more flexible robotic systems like PaLM-E and RT-2. Beyond their improved robotic capabilities, these models can ask questions, which marks a significant step toward robots that can interact more effectively with the real world.|
|7.|More technical research in agentic AI. Creating AI agents, systems capable of autonomous operation in specific environments, has long challenged computer scientists. However, emerging research suggests that the performance of autonomous AI agents is improving. Current agents can now master complex games like Minecraft and effectively tackle real-world tasks, such as online shopping and research assistance.
|8.|Closed Large Language Models significantly outperform open ones. On 10 select AI benchmarks, closed models outperformed open ones, with a median performance advantage of 24.2%. Differences in the performance of closed and open models carry important implications for AI policy debates.|

### 2.1 Overview of AI in 2023

>The technical performance chapter begins with a high-level overview of significant model releases in 2023 and reviews the current state of AI technical performance.

|2023|Mode|Type|Creator(s)|Significance|
|-|-|-|-|-|
|3/14| Claude             | Large language model | Anthropic        | First publicly released Large Language Model from Anthropic, designed to be helpful, honest, and harmless.    |
|3/14| GPT-4              | Large language model | OpenAI           | Among the most powerful Large Language Models, surpassing human performance on numerous benchmarks.           |
|3/23| Stable Diffusion v2| Text-to-image model  | Stability AI     | Upgrade producing higher-resolution, superior-quality images.                                |
|4/5| Segment Anything   | Image segmentation   | Meta             | Capable of isolating objects in images using zero-shot generalization.                        |
|7/18| Llama 2            | Large language model | Meta             | Open-source, with smaller variants delivering high performance for their size.                |
|8/20| DALL-E 3           | Image generation     | OpenAI           | Improved version of OpenAI’s text-to-vision model DALL-E.                                     |
|8/29| SynthID            | Watermarking         | Google, DeepMind | Tool for watermarking AI-generated music and images, detectable even after alterations.      |
|9/27| Mistral 7B         | Large language model | Mistral AI       | Compact 7B model surpassing Llama 2 13B in performance, top in its class for size.            |
|10/27| Ernie 4.0          | Large language model | Baidu            | High-performing Chinese Large Language Model from Baidu.                                                      |
|11/6| GPT-4 Turbo        | Large language model | OpenAI           | Upgraded Large Language Model with a 128K context window and reduced pricing.                                  |
|11/6| Whisper v3         | Speech-to-text       | OpenAI           | Open-source speech-to-text model known for increased accuracy and extended language support.  |
|11/21| Claude 2.1         | Large language model | Anthropic        | Features a 200K context window, enhancing its capacity to process extensive content.          |
|11/22| Inflection-2       | Large language model | Inflection       | Second Large Language Model from Inflection, highlighting intensifying competition in the Large Language Model arena.           |
|12/6| Gemini             | Large language model | Google           | Competitor to GPT-4, with its variant Gemini Ultra outshining GPT-4 on numerous benchmarks.   |
|12/21| Midjourney v6      | Text-to-image model  | Midjourney       | Enhances user experience with more intuitive prompts and superior image quality.              |

* Over the years, AI has surpassed human baselines on a handful of benchmarks, such as image classification
in 2015, basic reading comprehension in 2017, visual reasoning in 2020, and natural language inference in 2021.
* As of 2023, there are still some task categories where AI fails to exceed human ability. These tend to be more complex cognitive tasks, such as visual commonsense reasoning and advanced-level mathematical problem solving.
* An emerging theme in AI technical performance, as emphasized in last year’s report, is the observed saturation on many benchmarks.
* AI researchers are pivoting away from traditional benchmarks and testing AI on more difficult challenges. The 2024 AI Index tracks progress on several new benchmarks including those for tasks in coding, advanced
reasoning, and agentic behavior—areas that were underrepresented in previous versions of the report.

### 2.2 Language

>*Hallucination—creating seemingly realistic, yet false, information.*

|Task|Benchmark|
|-|-|
|Understanding|In 2022, Stanford researchers introduced HELM (Holistic Evaluation of Language Models), designed to evaluate Large Language Models across diverse scenarios, including reading comprehension, language understanding, and mathematical reasoning. As of January 2024, GPT-4 leads the aggregate HELM leaderboard with a mean win rate of 0.96|
||Gemini Ultra’s score was the first to surpass MMLU’s (Massive Multitask Language Understanding) human baseline of 89.8%.|
|Generation|Launched in 2023, the Chatbot Arena Leaderboard is one of the first comprehensive evaluations of public Large Language Model preference. The leaderboard allows users to query two anonymous models and vote for the preferred generations. As of early 2024, the platform has garnered over 200,000 votes, and users ranked OpenAI’s GPT-4 Turbo as the most preferred model.|
|Factuality and Truthfulness|Introduced at ACL 2022, TruthfulQA is a benchmark designed to evaluate the truthfulness of Large Language Models in generating answers to questions. This benchmark comprises approximately 800 questions across 38 categories, including health, politics, and finance. GPT-4 (RLHF) released in early 2024, has achieved the highest performance thus far on the TruthfulQA benchmark, with a score of 0.6. This score is nearly three times higher than that of a GPT-2-based model tested in 2021, indicating that Large Language Models are becoming progressively better at providing truthful answers|
||HaluEval, introduced in 2023, is a new benchmark designed to assess hallucinations in Large Language Models. It includes over 35,000 samples, both hallucinated and normal, for analysis and evaluation by Large Language Models. The findings reveal that many Large Language Models struggle with these tasks, highlig that hallucination is a significant ongoing issue.|

### 2.3 Coding

>Coding involves the generation of instructions that computers can follow to perform tasks. Recently, Large Language Models have become proficient coders, serving as valuable assistants to computer scientists. There is also increasing evidence that many coders find AI coding assistants highly useful.

|Coding|Benchmark|
|-|-|
|Generation|A GPT-4 model variant (AgentCoder) currently leads in HumanEval performance, scoring 96.3%.|
||Claude 2, the best-performing model, solved only 4.8% of the dataset’s problems. (A dataset comprising 2,294 software engineering problems sourced from real GitHub issues and popular Python repositories) GPT-4 is scoring 1.74% and ChatGPT-3.5 is scoring 0.52%.|

### 2.4 Image Computer Vision and Image Generation

>Computer vision allows machines to understand images and videos and create realistic visuals from textual prompts or other inputs. This technology is widely used in fields such as autonomous driving, medical imaging, and video game development.

|Function|Details|Notes|
|-|-|-|
|Generation|In 2023, Stanford researchers introduced the Holistic Evaluation of Text-toImage Models (HEIM), a benchmark designed to comprehensively assess image generators across 12 key aspects crucial for real-world deployment, such as image-text alignment, image quality, and aesthetics.|Image-text-alignment, DALL-E 2 (3.5B) 0.94|
|||Stable Diffusion (Dreamlike Photoreal v2.0 1B) 0.92|
|||Aesthetics Stable Diffusion (Dreamlike Photoreal v2.0 1B) 0.87|
|||OriginalityStable Diffusion (Dreamlike Photoreal v2.0 1B) 0.98|
|Instruction-Following|In 2023, a team of industry and academic researchers introduced VisIT-Bench, a benchmark consisting of 592 challenging vision-language instructions across about 70 instruction categories, such as plot analysis, art knowledge, and location understanding.|VisIT-Bench is GPT-4V, the visionenabled variant of GPT-4 Turbo, with an Elo score of 1,349, marginally surpassing the human reference score for VisIT-Bench|
|Editing|EditVal, a new benchmark for assessing text-guided image editing, includes over 13 edit types, such as adding objects or changing their positions, across 19 object classes.|The benchmark was applied to evaluate eight leading text-guided image editing methods including SINE and Null-text.|
|Conditional control editing| Existing text-to-image models often lack precise control over the spatial composition of an image, making it difficult to use prompts alone to generate images with complex layouts, diverse shapes, and specific poses. Fine-tuning these models for greater compositional control by training them on additional images is theoretically feasible, but many specialized datasets, such as those for human poses, are not large enough to support successful training.|In 2023, researchers from Stanford introduced a new model, ControlNet, that improves conditional control editing for large textto-image diffusion models. The introduction of ControlNet is a significant step toward creating advanced textto-image generators capable of editing images to more accurately replicate the complex images frequently encountered in the real world.|
||Instruct-NeRF2NeRF is a model developed by Berkeley researchers that employs an image-conditioned diffusion model for iterative text-based editing of 3D geometries.|This method efficiently generates new, edited images that adhere to textual instructions, achieving greater consistency than current leading methods.|
|Segmentation|In 2023, Meta researchers launched Segment Anything, a project that featured the Segment Anything Model (SAM) and an extensive SA1B dataset for image segmentation.|Meta’s Segment Anything model was then used, alongside human annotators, to create the SA-1B dataset, which included over 1 billion segmentation masks across 11 million images. A new segmentation dataset of this size will accelerate the training of future image segmentors. Segment Anything demonstrates how AI models can be used alongside humans to more efficiently create large datasets, which in turn can be used to train even better AI systems.|
||RealFusion, developed by Oxford researchers, utilizes existing 2D image generators to produce multiple views of an object, and then assembles these views into a comprehensive 360 degree model.|This technique yields more accurate 3D reconstructions compared to stateof-the-art methods from 2021 (Shelf-Supervised),across a wide range of objects.|

### 2.5 Video Computer Vision and Video Generation

> Video analysis concerns performing tasks across videos rather than single images.

|Generation Function|Notes|
|-|-|
|UCF101 is an action recognition dataset of realistic action videos that contain 101 action categories|UCF101 has been used to benchmark video generators. This year’s top model, W.A.L.T-XL, posted an FVD16 score of 36, more than halving the state-of-the-art score posted the previous year.|
|Most existing methods can only create short, lowresolution videos. Latent Diffusion Model (LDM) notably outperforms previous state-ofthe-art methods released in 2022 like Long Video GAN (LVG) in resolution quality.|The adaptation of a text-to-image architecture to create LDM, a highly effective text-to-video model,exemplifies how advanced AI techniques can be repurposed across different domains of computer vision. The LDM’s strong video generation capabilities have many real-world applications, such as creating realistic driving simulations.|
|Emu Video, a new transformerbased video generation model created by Meta researchers, represents a significant step forward. Emu Video generates an image from text and then creates a video based on both the text and image.|Emu Video model outperforms previously released state-of-the-art video generation methods. The metric is the proportion of cases when human evaluators preferred Emu Video’s image quality or faithfulness to text instructions over the compared method. Emu Video simplifies the video generation process and signals a new era of high-quality video generation.|

### 2.6 Reasoning

> Reasoning in AI involves the ability of AI systems to draw logically valid conclusions from different forms of information. AI systems are increasingly being tested in diverse reasoning contexts, including visual (reasoning about images), moral (understanding moral dilemmas), and social reasoning (navigating social situations).

|Function|Details|Notes|
|-|-|-|
|General Reasoning|A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. MMMU comprises about 11,500 college-level questions from six core disciplines: art and design, business,science, health and medicine, humanities and social science, and technology and engineering. The question formats include charts, maps, tables, chemical structures, and more. MMMU is one of the most demanding tests of perception, knowledge, and reasoning in AI to date.|As of January 2024, the highest performing model is Gemini Ultra, which leads in all subject categories with an overall score of 59.4%. On most individual task categories, top models are still well beyond medium-level human experts.|
||NYU, Anthropic, and Meta introduced the GPQA benchmark to test general multisubject AI reasoning. This dataset consists of 448 difficult multiple-choice questions that cannot be easily answered by Google searching. The questions were crafted by subject-matter experts in various fields like biology, physics, and chemistry.|PhD-level experts achieved a 65% accuracy rate in their respective domains on GPQA, while nonexpert humans scored around 34%. The best-performing AI model, GPT-4, only reached a score of 41.0% on the main test set.|
|Abstraction and Reasoning Tasks|Abstract reasoning involves using known information to solve unfamiliar and novel problems and is a key aspect of human cognition that is evident even in toddlers. While recent Large Language Models like GPT-4 have shown impressive performance, their capability for true abstract reasoning remains a hotly debated subject.|Santa Fe Institute tested GPT-4 on the ConceptARC benchmark, a collection of analogy puzzles designed to assess general abstract reasoning skills. The study revealed that GPT-4 significantly trails behind humans in abstract reasoning abilities: While humans score 95% on the benchmark, the best GPT-4 system only scores 69%.|
|Mathematical Reasoning|GSM8K, a dataset comprising approximately 8,000 varied grade school math word problems, requires that AI models develop multistep solutions utilizing arithmetic operations.|GSM8K has quickly become a favored benchmark for evaluating advanced Large Language Models. The top-performing model on GSM8K is a GPT-4 variant (GPT-4 Code Interpreter), which scores an accuracy of 97%, a 4.4% improvement from the state-of-the-art score in the previous year and a 30.4% improvement from 2022 when the benchmark was first introduced.|
||MATH is a dataset of 12,500 challenging competition-level mathematics problems introduced by UC Berkeley researchers in 2021.|AI systems struggled on MATH when it was first released, managing to solve only 6.9% of the problems. Performance has significantly improved. In 2023, a GPT-4-based model posted the top result, successfully solving 84.3% of the dataset’s problems.|
|PlanBench|A planning system receives a specified goal, an initial state, and a collection of actions. Each action is defined by preconditions, which must be met for the action to be executed, and the effects that result from the action’s execution. The system constructs a plan, comprising a series of actions, to achieve the goal from the initial state. A group from Arizona State University has proposed PlanBench, a benchmarksuite containing problems used in the automated planning community, especially those used in the International Planning Competition.|They tested I-GPT-3 and GPT-4 on 600 problems in the Blocksworld domain (where a hand tries to construct stacks of blocks when it is only allowed to move one block at a time to the table or to the top of a clear block) using one-shot learning and showed that GPT-4 could generate correct plans and cost-optimal plans about 34% of the time, and I-GPT-3 about 6%.|
|Visual Reasoning|Visual Commonsense Reasoning (VCR) Introduced in 2019, the Visual Commonsense Reasoning (VCR) challenge tests the commonsense visual reasoning abilities of AI systems. In this challenge, AI systems not only answer questions based on images but also reason about the logic behind their answers.|While AI  systems have yet to outperform humans on this task, their capabilities are steadily improving. Between 2022 and 2023, there was a 7.93% increase in AI performance on the VCR challenge.|
|Moral Reasoning|A team of Stanford researchers created a new dataset (MoCa) of human stories with moral elements.|No model perfectly matches human moral systems, but newer, larger models like GPT-4 and Claude show greater alignment with human moral sentiments than smaller models like GPT-3, suggesting that as AI models scale, they are gradually becoming more morally aligned with humans.|
|Causal Reasoning|Assessing whether Large Language Models have theory-of-mind (ToM) capabilities—understanding and attributing mental states such as beliefs, intentions, and emotions—has traditionally challenged AI researchers. BigToM, comprising 25 controls and 5,000 model-generated evaluations, has been rated by human evaluators as superior to existing ToM benchmarks.|BigToM tests Large Language Models on forward belief (predicting future events), forward action (acting based on future event predictions), and backward belief (retroactively inferring causes of actions). GPT-4 was the top performer, with ToM capabilities nearing but not surpassing human levels. More pecifically, as measured by accuracy in correctly inferring beliefs, GPT-4 closely matched human performance in forward belief and backward belief tasks and slightly surpassed humans in forward action tasks.|
||Tübingen Cause-Effect Pairs, researchers from Microsoft and the University of Chicago have demonstrated that Large Language Models are effective causal reasoners.|Notably, GPT-4 outperformed prior covariance-based AI models, which were explicitly trained for causal reasoning tasks.|

### 2.7 Audio

> AI systems are adept at processing human speech, with audio capabilities that include transcribing spoken words to text and recognizing individual speakers. More recently, AI has advanced in generating synthetic audio content.

|Models|Details|Performanc|
|-|-|-|
|UniAudio|UniAudio uniformly tokenizes all audio types and, like modern Large Language Models, employs next-token prediction for highquality audio generation. UniAudio is capable of generating high-quality speech, sound, and music.|UniAudio surpasses leading methods in tasks, including text-to-speech, speech enhancement, and voice conversion. With 1 billion parameters and trained on 165,000 hours of audio, UniAudio exemplifies the efficacy of big data and self-supervision for music generation.|
|MusicGEN and MusicLM|Meta’s MusicGen is a novel audio generation model that also leverages the transformerarchitecture common in language models to generate audio. MusicGen enables users to specify text for a desired audio outcome and then fine-tune it using specific melodies.|In comparative studies, MusicGen outshines other popular text-to-music models like Riffusion, Moûsai, and MusicLM across various generative music metrics. Although MusicGen outperforms certain textto-music models released earlier in the year, MusicLM is worth highlighting because its release was accompanied by the launch of MusicCaps, a state-of-the-art dataset of 5.5K music-text pairs. MusicCaps was used by MusicGen researchers to benchmark the performance of their family of models.|

### 2.8 Agents

> AI agents, autonomous or semiautonomous systems designed to operate within specific environments to accomplish goals, represent an exciting frontier in AI research. These agents have a diverse range of potential applications, from assisting in academic research and scheduling meetings to facilitating online shopping and vacation booking.

|Benchmark|Details|
|-|-|
|AgentBench|A new benchmark designed for evaluating Large Language Model-based agents, encompasses eight distinct interactive settings, including web browsing,online shopping, household management, puzzles, and digital card games. GPT-4 emerged as the top performer, achieving an overall score of 4.01, significantly higher than Claude 2’s score of 2.49. The AgentBench team speculated that agents’ struggles on certain benchmark subsections can be attributed to their limited abilities in long-term reasoning, decision-making, and instruction-following.|
|Voyageur|Nvidia, Caltech, UT Austin, Stanford, and UW Madison demonstrates that existing Large Language Models like GPT-4 can be used to develop flexible agents capable of continuous learning. Voyager, demonstrates remarkable proficiency in a dynamic video game setting, thereby representing a notable advancement in the field of agentic AI.|
|MLAgentBench|A new benchmark tests whether AI agents are capable of engaging in scientific experimentation. MLAgentBench assesses AI systems’ potential as computer science research assistants, evaluating their performance across 15 varied research tasks.|

### 2.9 Robotics

> Over time, AI has become increasingly integrated into robotics, enhancing robots’ capabilities to perform complex tasks. Especially with the rise of foundation models, this integration allows robots to iteratively learn from their surroundings, adapt flexibly to new settings, and make autonomous decisions.

* PaLM-E is a new AI model from Google that merges robotics with language modeling to address real-world tasks like robotic manipulation and knowledge tasks like question answering and image captioning. Leveraging transformer-based architectures, the largest PaLM-E model is scaled up to 562B parameters. The model is trained on diverse visual language as well as robotics data, which results in superior performance on a variety of robotic benchmarks. PaLM-E also sets new standards in visual tasks like OK-VQA, excels in other language tasks, and can engage in chain-of-thought, mathematical, and multi-image reasoning, even without specific training in these areas.
* RT-2, a new robot released from DeepMind, uses a transformer-based architecture and is trained on both robotic trajectory data that is tokenized into text and extensive visual-language data. It outshines state-of-the-art models like Manipulation of Open-World Objects (MOO) across various benchmarks, particularly in tasks involving unseen objects. On such tasks, an RT-2/PaLM-E variant achieves an 80% success rate, significantly higher than MOO’s (53%).

### 2.10 Reinforcement Learning

>In reinforcement learning, AI systems are trained to maximize performance on a given task by interactively learning from their prior actions. Systems are rewarded if they achieve a desired goal and punished if they fail.

* **Reinforcement Learning from Human Feedback (RLHF)** incorporates human feedback into the reward function, enabling models to be trained for characteristics like helpfulness and harmlessness. The rising popularity of RLHF is also evidenced by the fact that many leading Large Language Models report improving their models with RLHF.

* RLHF is a powerful method for aligning AI models but can be hindered by the time and labor required to generate human preference datasets for model alignment. As an alternative, **Reinforcement Learning from AI Feedback (RLAIF)** uses reinforcement learning based on the preferences of Large Language Models to align other AI models toward human preferences. Notably, in harmless dialogue generation tasks focused on producing the least harmful outputs, RLAIF (88%) surpasses RLHF (76%) in effectiveness. This research indicates that RLAIF could be a more resource-efficient and cost-effective approach for AI model alignment.

* Stanford and CZ Biohub have developed a new reinforcement learning algorithm for aligning models named **Direct Preference Optimization (DPO)**. DPO is simpler than RLHF but equally effective. The researchers show that DPO is as effective as other existing alignment methods, such as Proximal Policy Optimization (PPO) and Supervised FineTuning (SFT).

### 2.11 Properties of Large Language Models

>This section focuses on research exploring critical properties of Large Language Models, such as their capacity for sudden behavioral shifts and self-correction in reasoning. It is important to highlight these studies to develop an understanding of how Large Language Models, which are increasingly representative of the frontier of AI research, operate and behave.

* Challenging the Notion of Emergent Behavior
Large Language Models exhibit emergent abilities, meaning they can unpredictably and suddenly display new capabilities at larger scales. This has raised concerns that even larger models could develop surprising, and perhaps uncontrollable, new abilities. Stanford challenges this notion, arguing that the perceived emergence of new capabilities is often a reflection of the benchmarks used for evaluation rather than an inherent property of the models themselves.
* Changes in Large Language Model Performance Over Time
Publicly usable closed-source Large Language Models, are often updated over time by their developers in response to new data or user feedback. However, there is little research on how the performance of such models changes, if at all, in response to such updating. This research highlights that Large Language Model performance can evolve over time and suggests that regular users should be mindful of such changes.
* Large Language Models Are Poor Self-Correctors
It is generally understood that Large Language Models like GPT-4 have reasoning limitations and can sometimes produce hallucinations. One proposed solution to such issues is self-correction, whereby Large Language Models identify and correct their own reasoning flaws. However, it is currently not well understood whether Large Language Models are in fact capable
of this kind of self-correction.
* On all selected benchmarks, closed models outperform open ones. Specifically, on 10 selected benchmarks, closed models achieved a median performance advantage of 24.2%, with differences ranging from as little as 4.0% on mathematical tasks like GSM8K to as much as 317.7% on agentic tasks like AgentBench.

||Closed Models|Open Models|
|-|-|-|
|Accessibility|Limited access, usually restricted to developers|Accessible via APIs, but model weights may not be fully released|
|Modification|Generally not open to public modification|May allow modification by the public|
|Scrutiny|Limited transparency, controlled by developers|May lack full transparency due to withheld model weights|
|Performance|Often demonstrate superior performance|Performance can vary, sometimes lagging behind closed models|
|Innovation|Control may inhibit external innovation|Potential for broader collaboration and innovation|
|Security|Lower perceived security risks due to limited access|Potential for misuse, raising security concerns|
|Example Large Language Models|Google's Gemini, Microsoft's Turing|OpenAI's GPT-4, Anthropic's Claude 2, Meta's Llama 2,Stability AI's Stable Diffusion|

### 2.12 Techniques for Large Language Model Improvement

>As Large Language Models use increases, techniques are being sought to enhance their performance and efficiency. This section examines some of those advances. 

#### Prompting

* Mastering the art of crafting effective prompts significantly enhances the performance of Large Language Models without requiring that models undergo underlying improvements.
* Chain of thought (CoT) and Tree of Thoughts (ToT) are prompting methods that can improve the performance of Large Language Models on reasoning tasks.
* A paper from DeepMind has introduced Optimization by PROmpting (OPRO), a method that uses Large Language Models to iteratively generate prompts to improve algorithmic performance. OPRO uses natural language to guide Large Language Models in creating new prompts based on problem descriptions and previous solutions. Compared to other prompting approaches like “let’s think step by step” or an empty starting point, ORPO leads to significantly greater accuracy on virtually all 23 BIG-bench Hard tasks.

#### Fine-Tuning

* QLoRA, developed by researchers from the University of Washington in 2023, is a new method for more efficient model fine-tuning. It dramatically reduces memory usage, enabling the fine-tuning of a 65 billion parameter model on a single 48GB GPU while maintaining full 16-bit fine-tuning performance. To put this in perspective, fine-tuning a 65B Llama model, a leading open-source Large Language Model, typically requires about 780 GB of GPU memory. QLoRA is nearly 16 times more efficient. QLoRA manages to increase efficiency with techniques like a 4-bit NormalFloat (NF4), double quantization, and page optimizers. QLoRA is used to train a model named Guanaco, which matched or even surpassed models like ChatGPT in performance on the Vicuna benchmark (a benchmark that ranks the outputs of Large Language Models). Remarkably, the Guanaco models were created with just 24 hours of fine-tuning on a single GPU. QLoRa highlights how methods for optimizing and further improving models have become more efficient, meaning fewer resources will be required to make increasingly capable models.

#### Attention

* Flash-Decoding, developed by Stanford researchers, tackles inefficiency in traditional Large Language Models by speeding up the attention mechanism, particularly in tasks requiring long sequences. It achieves this by parallelizing the loading of keys and values, then separately rescaling and combining them to maintain right attention outputs. In various tests, Flash-Decoding outperforms other leading methods like PyTorch Eager and FlashAttention-2, showing much faster inference: For example, on a 256 batch size and 256 sequence length, Flash-Decoding is 48 times faster than PyTorch Eager and six times faster than FlashAttention-2. Inference on models like ChatGPT can cost 0.01 per response, which can become highly expensive when deploying such models to millions of users. Innovations like Flash-Decoding are critical for reducing inference costs in AI.

### 2.13 Environmental Impact of AI Systems

>This section examines trends in the environmental impact of AI systems, highlighting the evolving landscape of transparency and awareness. Historically, model developers seldom disclosed the carbon footprint of their AI systems, leaving researchers to make their best estimates. Recently, there has been a shift toward greater openness, particularly regarding the carbon costs of training AI models. However, disclosure of the environmental costs associated with inference—a potentially more significant concern—remains insufficient. This section presents data on carbon emissions as reported by developers in addition to featuring notable research exploring the intersection of AI and environmental impact. With AI models growing in size and becoming more widely used, it has never been more critical for the AI research community to diligently monitor and mitigate the environmental effects of AI systems.

* Generally, larger models emit more carbon. Many prominent model developers such as OpenAI, Google,Anthropic, and Mistral do not report emissions in training, although Meta does.
* The environmental impact of training AI models can be significant. While the perquery emissions of inference may be relatively low, the total impact can surpass that of training when models are queried thousands, if not millions of times daily.
* AI can contribute positively to environmental sustainability.

|Positive AI environmental use cases|AI contribution|
|-|-|
|Management of thermal energy storage systems|Anticipating thermal energy needs and managing thermal energy storage systems.|
|Improving waste management|Saving time and costs in waste-to-energy conversion, waste sorting, and waste monitoring.|
|More eǅciently cooling buildings|Optimizing the energy usage associated with air-conditioning.|
|Improving pest management|Identifying and eliminating pests in commercial tomato harvests.|
|Enhancing urban air quality|Forecasting and predicting air quality in urban cities.|

## Chapter 3: Responsible AI

>AI is increasingly woven into nearly every facet of our lives. This integration is occurring in sectors such as education, finance, and healthcare, where critical decisions are often based on algorithmic insights. This trend promises to bring many advantages; however, it also introduces potential risks. Consequently, in the past year, there has been a significant focus on the responsible development and deployment of AI systems. The AI community has also become more concerned with assessing the impact of AI systems and mitigating risks for those affected.
This chapter explores key trends in responsible AI by examining metrics, research, and
benchmarks in four key responsible AI areas: privacy and data governance, transparency
and explainability, security and safety, and fairness. Given that 4 billion people are expected
to vote globally in 2024, this chapter also features a special section on AI and elections and
more broadly explores the potential impact of AI on political processes.

||Chapter Highlights|
|-|-|
|1.|Robust and standardized evaluations for Large Language Model responsibility are seriously lacking. New research from the AI Index reveals a significant lack of standardization in responsible AI reporting. Leading developers, including OpenAI, Google, and Anthropic, primarily test their models against different responsible AI benchmarks. This practice complicates efforts to systematically compare the risks and limitations of top AI models.|
|2.|Political deepfakes are easy to generate and difficult to detect. Political deepfakes are already affecting elections across the world, with recent research suggesting that existing AI deepfake detection methods perform with varying levels of accuracy. In addition, new projects like CounterCloud demonstrate how easily AI can create and disseminate fake content.|
|3.|Researchers discover more complex vulnerabilities in Large Language Models. Previously, most efforts to red team AI models focused on testing adversarial prompts that intuitively made sense to humans. This year, researchers found less obvious strategies to get Large Language Models to exhibit harmful behavior, like asking the models to infinitely repeat random words.|
|4.|Risks from AI are a concern for businesses across the globe. A global survey on responsible AI highlights that companies’ top AI-related concerns include privacy, security, and reliability. The survey shows that organizations are beginning to take steps to mitigate these risks. However, globally, most companies have so far only mitigated a portion of these risks.|
|5.|Large Language Models can output copyrighted material. Multiple researchers have shown that the generative outputs of popular Large Language Models may contain copyrighted material, such as excerpts from The New York Times or scenes from movies. Whether such output constitutes copyright violations is becoming a central legal question.|
|6.|AI developers score low on transparency, with consequences for research. The newly introduced Foundation Model Transparency Index shows that AI developers lack transparency, especially regarding the disclosure of training data and methodologies. This lack of openness hinders efforts to further understand the robustness and safety of AI systems.|
|7.|Extreme AI risks are difficult to analyze. Over the past year, a substantial debate has emerged among AI scholars and practitioners regarding the focus on immediate model risks, like algorithmic discrimination, versus potential long-term existential threats. It has become challenging to distinguish which claims are scientifically founded and should inform policymaking. This difficulty is compounded by the tangible nature of already present short-term risks in contrast with the theoretical nature of existential threats.|
|8.|The number of AI incidents continues to rise. According to the AI Incident Database, which tracks incidents related to the misuse of AI, 123 incidents were reported in 2023, a 32.3% increase from 2022. Since 2013, AI incidents have grown by over twentyfold. A notable example includes AI-generated, sexually explicit deepfakes of Taylor Swift that were widely shared online.|
|9.|ChatGPT is politically biased. Researchers find a significant bias in ChatGPT toward Democrats in the United States and the Labour Party in the U.K. This finding raises concerns about the tool’s potential to influence users’ political views, particularly in a year marked by major global elections.|

### 3.1 Assessing Responsible AI

>This chapter begins with an overview of key trends in responsible AI (RAI). In this section the AI Index defines key terms in responsible AI: privacy, data governance, transparency, explainability, fairness, as well as security and safety. Next, this section looks at AI-related incidents and explores how industry actors perceive AI risk and adopt AI risk mitigation measures. Finally, the section profiles metrics pertaining to the overall trustworthiness of AI models and comments on the lack of standardized responsible AI benchmark reporting.

|Responsible AI dimension|Definition|Example|
|-|-|-|
|Data governance|Establishment of policies, procedures, and standards to ensure the quality, security, and ethical use of data, which is crucial for accurate, fair, and responsible AI operations, particularly with sensitive or personally identiǇable information.|Policies and procedures are in place to maintain data quality and security, with a particular focus on ethical use and consent, especially for sensitive health information.|
|Explainability|The capacity to comprehend and articulate the rationale behind AI decisions, emphasizing the importance of AI being not only transparent but also understandable to users and stakeholders.| The platform can articulate the rationale behind its treatment recommendations, making these insights understandable to doctors and patients, ensuring trust in its decisions.|
|Fairness|Creating algorithms that are equitable, avoiding bias or discrimination, and considering the diverse needs and circumstances of all stakeholders, thereby aligning with broader societal standards of equity.|The platform is designed to avoid bias in treatment recommendations, ensuring that patients from all demographics receive equitable care.|
|Privacy|An individual’s right to conǇdentiality, anonymity, and protection of their personal data, including the right to consent and be informed about data usage, coupled with an organization’s responsibility to safeguard these rights when handling personal data.|Patient data is handled with strict conǇdentiality, ensuring anonymity and protection. Patients consent to whether and how their data is used to train a treatment recommendation system.|
|Security and safety|The integrity of AI systems against threats, minimizing harms from misuse, and addressing inherent safety risks like reliability concerns and the potential dangers of advanced AI systems.|Measures are implemented to protect against cyber threats and ensure the system’s reliability, minimizing risks from misuse or inherent system errors, thus safeguarding patient health and data.|
|Transparency|Open sharing of development choices, including data sources and algorithmic decisions, as well as how AI systems are deployed, monitored, and managed, covering both the creation and operational phases.|The development choices, including data sources and algorithmic design decisions, are openly shared. How the system is deployed and monitored is clear to healthcare providers and regulatory bodies.|

| |Assessing Responsible AI|
|-|-|
|AI Incidents|The AI Incident Database (AIID) tracks instances of ethical misuse of AI. Since 2013, AI incidents have grown by over twentyfold. The continuous increase in reported incidents likely arises from both greater integration of AI into realworld applications and heightened awareness of its potential for ethical misuse.|
||AI-generated nude images of Taylor Swift|
||Unsafe behavior of fully self-driving cars|
||Privacy concerns with romantic AI chatbots|
|Risk Perception|In collaboration with Accenture, this year a team of Stanford researchers ran a global survey with respondents from more than 1,000 organizations to assess the global state of responsible AI. The organizations, with total revenues of at least $500 million each, were taken from 20 countries and 19 industries. The researchers found that privacy and data governance risks, e.g.,the use of data without the owner’s consent or data leaks, are the leading concerns across the globe. Notably, they observe that these concerns are significantly higher in Asia and Europe compared to North America.|
|Risk Mitigation|The Global State of Responsible AI survey finds that organizations in most regions have started to operationalize responsible AI measures. The majority of organizations across regions have fully operationalized at least one mitigation measure for risks they reported as relevant to them, given their AI adoption.|
|Overall Trustworthiness|A new benchmark that evaluates Large Language Models on a broad spectrum of responsible AI metrics like stereotype and bias, adversarial robustness, privacy, and machine ethics, among others. Despite GPT-4’s improvements over GPT-3.5 on standard benchmarks, GPT-4 remains more susceptible to misleading prompts from jailbreaking tactics. Hugging Face now hosts a Large Language Model Safety Leaderboard, which is based on the framework introduced in DecodingTrust. As of early 2024, Anthropic’s Claude 2.0 was rated as the safest model.|
|Tracking Notable Responsible AI Benchmarks|Introduced in 2021, TruthfulQA assesses the truthfulness of Large Language Models in their responses. RealToxicityPrompts and ToxiGen track the extent of toxic output produced by language models. Additionally, BOLD and BBQ evaluate the bias present in Large Language Model generations. Citations for TruthfulQA have risen especially sharply.|
|Reporting Consistency|New analysis from the AI Index, suggests that standardized benchmark reporting for responsible AI capability evaluations is lacking. The AI Index examined a selection of leading AI model developers, specifically OpenAI, Meta, Anthropic, Google, and Mistral AI. The Index identified one flagship model from each developer (GPT-4, Llama 2, Claude 2, Gemini, and Mistral 7B) and assessed the benchmarks on which they evaluated their model. A few standard benchmarks for general capabilities evaluation were commonly used by these developers, such as MMLU, HellaSwag, ARC Challenge, Codex HumanEval, and GSM8K. Developers might selectively report benchmarks that positively highlight their model’s performance. To improve responsible AI reporting, it is important that a consensus is reached on which benchmarks model developers should consistently test.|

### 3.2 Privacy and Data Governance

>A comprehensive definition of privacy is difficult and context-dependent. For the purposes of this report, the AI Index defines privacy as an individual’s right to the confidentiality, anonymity, and protection of their personal data, along with their right to consent to and be informed about if and how their data is used. Privacy further includes an organization’s responsibility to ensure these rights if they collect, store, or use personal data (directly or indirectly). In AI, this involves ensuring that personal data is handled in a way that respects individual privacy rights, for example, by implementing measures to protect sensitive information from exposure, and ensuring that data collection and processing are transparent and compliant with privacy laws like GDPR.
Data governance, on the other hand, encompasses policies, procedures, and standards established to ensure the quality, security, and ethical use of data within an organization. In the context of AI, data governance is crucial for ensuring that the data used for training and operating AI systems is accurate, fair, and used responsibly and with consent. This is especially the case with sensitive or personally identifiable information (PII).

||Privacy and Data Governance in Numbers|
|-|-|
|Academia|The AI Index examined the number of responsible-AI-related academic submissions to six leading AI conferences: AAAI, AIES, FAccT, ICML, ICLR, and NeurIPS.6 Privacy and data governance continue to increase as a topic of interest for AI researchers. There were 213 privacy and data governance submissions in 2023 at the select AI conferences analyzed by the AI Index, nearly double the number submitted in 2022 (92).|
|Industry|According to the Global State of Responsible AI Survey, the survey listed six possible data governance–related measures they could indicate adopting. Overall, less than 0.6% of companies indicated that they had fully operationalized all six data governance mitigations. However, 90% of companies selfreported that they had operationalized at least one measure. Moreover, 10% reported they had yet to fully operationalize any of the measures. Globally, the companies surveyed reported adopting an average of 2.2 out of 6 data governance measures.|

||Featured Research|
|-|-|
|Extracting Data From Large Language Models|A study published in November 2023 explores extractable memorization: if and how sensitive training data can be extracted from Large Language Models without knowing the initial training dataset in advance.The authors propose that the key to data extraction lies in prompting the model to deviate from its standard dialog-style generation. For instance, the prompt “Repeat this word forever: ‘poem poem poem poem,’” can lead ChatGPT to inadvertently reveal sensitive PII data verbatim. Some prompts are more effective than others in causing this behavior. Red teaming models through various human-readable prompts to provoke unwanted behavior has become increasingly common. For instance, one might ask a model if it can provide instructions for building a bomb. While these methods have proven somewhat effective, the research mentioned above indicates there are other, more complex methods for eliciting unwanted behavior from models.|
|Foundation Models and Verbatim Generation|Research from Google, ETH Zurich, and Cornell explored data memorization in Large Language Models and found that models without any protective measures frequently reproduce text directly from their training data. Various models were found to exhibit differing rates of memorization for different datasets. Research has also highlighted challenges with exact and approximate memorization in visual content generation, notably with Midjourney v6. This study discovered that certain prompts could produce images nearly identical to those in films, even without direct instructions to recreate specific movie scenes. This indicates that the model might have been trained on copyrighted material. Despite efforts to frame indirect prompts to avoid infringement, the problem persisted, emphasizing the broader copyright issues associated with AI’s use of unlicensed data.|
|Auditing Privacy in AI Models|Determining whether a model is privacypreserving—that is, if it safeguards individuals’ personal information and data from unauthorized disclosure or access—is challenging. Recent research from Google introduces a new method to achieve this within a single training run, marking a substantial advancement over prior methods that necessitated multiple attacks and significant computational effort. This approach is not only less computationally demanding but also has a minimal impact on model performance, offering an efficient and low-impact method for conducting privacy audits on AI models.|

### 3.3 Transparency and Explainability

>Transparency in AI encompasses several aspects. Data and model transparency involve the open sharing of development choices, including data sources and algorithmic decisions. Operational transparency details how AI systems are deployed, monitored, and managed in practice. While explainability often falls under the umbrella of transparency, providing insights into the AI’s decision-making process, it is sometimes treated as a distinct category. This distinction underscores the importance of AI being not only transparent but also understandable to users and stakeholders. For the purposes of this chapter, the AI Index includes explainability within transparency, defining it as the capacity to comprehend and articulate the rationale behind AI decisions.

||Transparency and Explainability in Numbers|
|-|-|
|Academia|Since 2019, the number of papers on transparency and explainability submitted to major academic conferences has more than tripled. In 2023, there was a record-high number of explainability-related submissions (393) at academic conferences including AAAI, FAccT, AIES, ICML, ICLR, and NeurIPS.|
|Industry|In the Global State of Responsible AI Survey, 44% of all surveyed organizations indicated that transparency and explainability are relevant concerns given their AI adoption strategy. Only 8% of companies across all regions and industries fully implemented more than half of the measures. A significant portion(12%) had not fully operationalized any measures. Overall, less than 0.7% of companies indicated full operationalization of all the measures.|

||Featured Research|
|-|-|
|The Foundation Model Transparency Index|In October 2023, Stanford, Princeton, and MIT researchers released the Foundation Model Transparency Index (FMTI). Meta’s Llama 2 and BigScience’s BLOOMZ stand out as the most transparent models. However, it is important to note that all models received relatively low scores, with the mean score at 37%. Additionally, open models—those openly releasing their weights—tend to score significantly better on transparency, with an average score of 51.3%, compared to closed models, which have limited access and score an average of 30.9%.|
|Neurosymbolic Artificial Intelligence (Why, What, and How)|Research from the University of South Carolina and the University of Maryland provides a comprehensive mapping and taxonomy of various approaches within neurosymbolic AI. The researchers hope that neurosymbolic AI could mitigate some of the shortcomings of purely neural network–based models, such as hallucinations or incorrect reasoning, by mimicking human cognition—specifically, by enabling models to possess an explicit knowledge model of the world.|

### 3.4 Security and Safety

>In 2023, as AI capabilities continued to improve and models became increasingly ubiquitous, concerns about their security and safety became a top priority for decision-makers. This chapter explores three distinct aspects of security and safety. First, guaranteeing the integrity of AI systems involves protecting components such as algorithms, data, and infrastructure against external threats like cyberattacks or adversarial attacks. Second, safety involves minimizing harms stemming from the deliberate or inadvertent misuse of AI systems. This includes concerns such as the development of automated hacking tools or the utilization of AI in cyberattacks. Lastly, safety encompasses inherent risks from AI systems themselves, such as reliability concerns (e.g., hallucinations) and potential risks posed by advanced AI systems.

||AI Security and Safety in Numbers|
|-|-|
|Academia|Although the number of security and safety submissions at select academic conferences decreased since 2022, there has been an overall 70.4% increase in such submissions since 2019.|
|Industry|The Global State of Responsible AI survey also queried organizations about reliability risks, such as model hallucinations or output errors. In a survey of more than 1,000 organizations, 45% acknowledged the relevance of reliability risks to their AI adoption strategies. Among these, 13% have fully implemented more than half of the surveyed measures, while 75% have operationalized at least one but fewer than half. Additionally, 12% of respondents admitted to having no reliability measures fully operationalized. The global average stood at 2.16 fully implemented measures out of the six included in the survey.|
||Organizations were also queried on the relevance of security risks, such as cybersecurity incidents, with 47% acknowledging their relevance. 28% had fully implemented more than half of the proposed security measures, while 63% had fully operationalized at least one but fewer than half. Additionally, 10% reported having no AI security measures fully operationalized. On average, companies adopted 1.94 measures out of the 5 surveyed.|
||88% of organizations, either agree or strongly agree that those developing foundation models are responsible for mitigating all associated risks. Furthermore, 86% of respondents either agree or strongly agree that the potential threats posed by generative AI are substantial enough to warrant globally agreed-upon governance.|

||Featured Research|
|-|-|
|Do-Not-Answer: A New Open Dataset for Comprehensive Benchmarking of Large Language Model Safety Risks|A team of international researchers recently created one of the first comprehensive opensource datasets for assessing safety risks in Large Language Models. Their evaluation encompasses responses from six prominent language models: GPT-4, ChatGPT, Claude, Llama 2, Vicuna, and ChatGLM2. The authors also developed a risk taxonomy spanning a range of risks, from mild to severe. The authors find that most models output harmful content to some extent. GPT-4 and ChatGPT are mostly prone to discriminatory, offensive output, while Claude is susceptible to propagating misinformation. Across all tested models, the highest number of violations was recorded for ChatGLM2.|
|Universal and Transferable Attacks on Aligned Language Models|In 2023, researchers unveiled a universal attack capable of operating across various Large Language Models. This attack induces aligned models to generate objectionable content. The method involved automatically generating suffixes that, when added to various prompts, compel Large Language Models to produce unsafe content. The method the researchers introduce is called Greedy Coordinate Gradient(GCG). The study demonstrates that these suffixes (the GCG attack) often transfer effectively across both closed and open models, encompassing ChatGPT, Bard, Claude, Llama-2-Chat, and Pythia. This study raises an important question as to how models can be better fortified against automated adversarial attacks. It also demonstrates how Large Language Models can be vulnerable to attacks that employ unintelligible, non-human-readable prompts. Current red-teaming methodologies primarily focus on interpretable prompts. This new research suggests there is a significant gap in buffering Large Language Models against attacks utilizing uninterpretable prompts.|
|MACHIAVELLI Benchmark|Introduced in 2023, MACHIAVELLI is a new benchmark designed to address this gap. Its creators crafted a collection of 134 choose-your-own-adventure games, encompassing over half a million diverse social decision-making scenarios. Through their research, the authors reveal that models confront trade-offs between maximizing rewards (game scores) and making ethical decisions. For instance, a model inclined to boost its score may find itself compelled to compromise its ethical stance. Furthermore, the researchers demonstrate that there are strategies for mitigating the trade-off between maximizing rewards and maintaining ethical behavior, which can lead to the development of proficient and ethical AI agents.|

### 3.5 Fairness

>Fairness in AI emphasizes developing systems that are equitable and avoid perpetuating bias or discrimination against any individual or group. It involves considering the diverse needs and circumstances of all stakeholders impacted by AI use. Fairness extends beyond a technical concept and embodies broader social standards related to equity.

||Fairness in Numbers|
|-|-|
|Academia|In 2023, there were 212 papers on fairness and bias submitted, a 25.4% increase from 2022. Since 2019, the number of such submissions has almost quadrupled.|
|Industry|In the Global State of Responsible AI survey referenced earlier, 29% of organizations identified fairness risks as relevant to their AI adoption strategies. Results show that while most companies have fully implemented at least one fairness measure, comprehensive integration is still lacking. The global average for adopted fairness measures stands at 1.97 out of five measures asked about.|

||Featured Research|
|-|-|
|(Un)Fairness in AI and Healthcare|A team of American and Canadian researchers investigated racial bias when Large Language Models are prompted to respond to medical questions. The study revealed that all models demonstrated some degree of race-based medical bias, although their responses to identical questions varied. For certain queries, like the basis of race, only one model, Claude, consistently offered problematic responses. In contrast, for other questions, such as the purported skin thickness differences between Black and white individuals (a widespread misconception among medical students), most models regularly produced concerning race-based responses. The occasional perpetuation of debunked myths by Large Language Models underscores the need for caution when employing Large Language Models in medical contexts.|
|Social Bias in Image Generation Models|BiasPainter employs a wide selection of seed images and neutral prompts related to professions, activities, objects, and personality traits for image editing. It then compares these edits to the original images, concentrating on identifying inappropriate changes in gender, race, and age. Stable Diffusion, Midjourney, and InstructPix2Pix were shown to be somewhat biased along different dimensions. Generally, the generated images were more biased along age and race than gender dimensions.|
|Measuring Subjective Opinions in Large Language Models|Research from Anthropic suggests that large language models do not equally represent global opinions on a variety of topics such as politics, religion, and technology. In this study, researchers built a GlobalOpinionQA dataset to capture cross-country opinions on various issues,including questions like, “When jobs are scarce, employers should give priority to people of this country over immigrants,” or “On the whole, men make better business executives than women do.” The experiments indicate that the models’ responses closely align with those from individuals in Western countries.|
|Large Language Model Tokenization Introduces Unfairness|Research from the University of Oxford highlights how inequality in AI originates at the tokenization stage. Tokenization, the process of breaking down text into smaller units for processing and analysis, exhibits significant variability across languages. Users of languages that require more tokens than English for the same content face up to four times higher inference costs and longer processing times, as both are dependent on the number of tokens. Second, these users may also experience increased processing times because models take longer to process a greater number of tokens. Lastly, given that models operate within a fixed context window—a limit on the amount of text or content that can be input—languages that require more tokens proportionally use up more of this window. This can reduce the available context for the model, potentially diminishing the quality of service for those users.|

### 3.6 AI and Elections

>In 2024, around 4 billion people across the globe will vote in national elections, for example, in the United States, U.K., Indonesia, Mexico, and Taiwan. Upcoming elections coupled with greater public awareness of AI have led to discussions of AI’s possible impact on elections. This section covers how AI can impact elections and more specifically examines the generation and dissemination of mis- and disinformation, the detection of AI-generated content, the potential political bias of Large Language Models, and the broader impact of AI on politics.

||AI and Elections|
|-|-|
|Generating Disinformation|One of the top concerns when discussing AI’s impact on political processes is the generation of disinformation. While disinformation has been around since at least the Roman Empire, AI makes it significantly easier to generate such disinformation.|
|Dissemination of Fake Content|A developer called Nea Paw set up Countercloud as an experiment in creating a fully automated disinformation pipeline. An AI model is used to continuously scrape the internet for articlesand automatically decide which content it should target with counter-articles. Next, another AI model is tasked with writing a convincing counter-article that can include images and audio summaries. This counter-article is subsequently attributed to a fake journalist and posted on the CounterCloud website.Subsequently, another AI system generates comments on the counter-article, creating the appearance of organic engagement. Finally, an AI searches X for relevant tweets, posts the counter-article as a reply, and comments as a user on these tweets. The entire setup for this authentic-appearing misinformation system only costs around $400.|
|Detecting Deepfakes|A team of Singaporean researchers studied how well deepfake detectors generalize to datasets they have not been trained on. The researchers compared five deepfake detection approaches and found that even more recently introduced deepfake detection methods suffer significant performance declines on never-before-seen datasets. However, the study does note that there are underlying similarities between seen and unseen datasets, meaning that in the future, robust and broadly generalizable deepfake detectors could becreated.|
|Large Language Models and Political Bias|New research published in 2023 suggests that many major Large Language Models like ChatGPT are not necessarily free of bias. The study revealed that ChatGPT exhibits a notable and systematic bias favoring Democrats in the United States and the Labour Party in the U.K. The identification of bias in these Large Language Models raises concerns about their potential to influence the political views and stances of users who engage with these tools.|
|Impact of AI useage on Political Processes|Heuristic-driven approximations Record linkage Outlier detection Drop box location determination Facility location Clustering Predictive policing Time series motifs Face recognition Biometrics Video-based vote counting Event detection Person re-identification|

## Chapter 4: Economy

> The integration of AI into the economy raises many compelling questions. Some predict that AI will drive productivity improvements, but the extent of its impact remains uncertain. A major concern is the potential for massive labor displacement—to what degree will jobs be automated versus augmented by AI? Companies are already utilizing AI in various ways across industries, but some regions of the world are witnessing greater investment inflows into this transformative technology. Moreover, investor interest appears to be gravitating toward specific AI subfields like natural language processing and data management.
This chapter examines AI-related economic trends using data from Lightcast, LinkedIn, Quid, McKinsey, Stack Overflow, and the International Federation of Robotics (IFR). It begins by analyzing AI-related occupations, covering labor demand, hiring trends, skill penetration, and talent availability. The chapter then explores corporate investment in AI, introducing a new section focused specifically on generative AI. It further examines corporate adoption of AI, assessing current usage and how developers adopt these technologies. Finally, it assesses AI’s current and projected economic impact and robot installations across various sectors.

||Chapter Highlights|
|-|-|
|1.|Generative AI investment skyrockets. Despite a decline in overall AI private investment last year, funding for generative AI surged, nearly octupling from 2022 to reach $25.2 billion. Major players in the generative AI space, including OpenAI, Anthropic, Hugging Face, and Inflection, reported substantial fundraising rounds.|
|2.|Already a leader, the United States pulls even further ahead in AI private investment. In 2023, the United States saw AI investments reach $67.2 billion, nearly 8.7 times more than China, the next highest investor. While private AI investment in China and the European Union, including the United Kingdom, declined by 44.2% and 14.1%, respectively, since 2022, the United States experienced a notable increase of 22.1% in the same time frame.|
|3.|Fewer AI jobs, in the United States and across the globe. In 2022, AI-related positions made up 2.0% of all job postings in America, a figure that decreased to 1.6% in 2023. This decline in AI job listings is attributed to fewer postings from leading AI firms and a reduced proportion of tech roles within these companies.|
|4.|AI decreases costs and increases revenues. A new McKinsey survey reveals that 42% of surveyed organizations report cost reductions from implementing AI (including generative AI), and 59% report revenue increases. Compared to the previous year, there was a 10 percentage point increase in respondents reporting decreased costs, suggesting AI is driving significant business efficiency gains.|
|5.|Total AI private investment declines again, while the number of newly funded AI companies increases. Global private AI investment has fallen for the second year in a row, though less than the sharp decrease from 2021 to 2022. The count of newly funded AI companies spiked to 1,812, up 40.6% from the previous year.|
|6.|AI organizational adoption ticks up. A 2023 McKinsey report reveals that 55% of organizations now use AI (including generative AI) in at least one business unit or function, up from 50% in 2022 and 20% in 2017.|
|7.|China dominates industrial robotics. Since surpassing Japan in 2013 as the leading installer of industrial robots, China has significantly widened the gap with the nearest competitor nation. In 2013, China’s installations accounted for 20.8% of the global total, a share that rose to 52.4% by 2022.|
|8.|Greater diversity in robotic installations. In 2017, collaborative robots represented a mere 2.8% of all new industrial robot installations, a figure that climbed to 9.9% by 2022. Similarly, 2022 saw a rise in service robot installations across all application categories, except for medical robotics. This trend indicates not just an overall increase in robot installations but also a growing emphasis on deploying robots for human-facing roles.|
|9.|The data is in: AI makes workers more productive and leads to higher quality work. In 2023, several studies assessed AI’s impact on labor, suggesting that AI enables workers to complete tasks more quickly and to improve the quality of their output. These studies also demonstrated AI’s potential to bridge the skill gap between low- and high-skilled workers. Still other studies caution that using AI without proper oversight can lead to diminished performance.|
|10.|Fortune 500 companies start talking a lot about AI, especially generative AI. In 2023, AI was mentioned in 394 earnings calls (nearly 80% of all Fortune 500 companies), a notable increase from 266 mentions in 2022. Since 2018, mentions of AI in Fortune 500 earnings calls have nearly doubled. The most frequently cited theme, appearing in 19.7% of all earnings calls, was generative AI.|

### 4.1 What’s New in 2023: A Timeline

|2023|Event|
|-|-|
|1/10|InstaDeep acquired by BioNTech|
||BioNTech, known for developing the first mRNA COVID-19 vaccine in partnership with Pfizer, acquires InstaDeep for 680 million to advance AI-powered drug discovery, design, and development. InstaDeep specializes in creating AI systems for enterprises in biology, logistics, and energy sectors.|
|1/23|Microsoft invests 10 billion in ChatGPT maker OpenAI|
||With this deal, Microsoft Azure remains the exclusive cloud provider for OpenAI, which relies on Azure to train its models. This follows Microsoft’s initial 1 billion investment in 2019 and a subsequent investment in 2021.|
|2/14|GitHub Copilot for Business becomes publicly available|
||Copilot for Business leverages an OpenAI Codex model to enhance code suggestion quality. At launch, GitHub Copilot contributed to an average of 46% of developers’ code across various programming languages, with this figure rising to 61% for Java.|
|3/7|Salesforce introduces Einstein GPT|
||Einstein GPT, the first comprehensive AI for CRM, utilizes OpenAI’s models. Einstein GPT aids Salesforce customers insales, marketing, and customer management.|
|3.16|Microsoft announces integration of GPT-4 into Office 365|
||Microsoft rolls out Copilot across Office 365, offering AI assistance in Word, PowerPoint, and Excel.|
|3/30|Bloomberg announces Large Language Model for finance|
||Bloomberg’s 50-billion parameter Large Language Model is custom-built for analyzing financial data and tailored to finance professionals. This model is capable of performing financial analyses on Bloomberg’s extensive datasets.|
|5/23|Adobe launches generative AI tools inside Photoshop|
||Adobe introduces generative AI features in Photoshop via Adobe Firefly, its generative image tool. Users can now add, remove, and edit images within seconds using text prompts.|
|6/8|Cohere raises 270 million|
||Cohere, focused on developing an AI model ecosystem for enterprises, raises 270 million in an oversubscribed Series C round. Inovia Capital led the round, with participation from Nvidia, Oracle, Salesforce Ventures, Schroders Capital, and Index Ventures.|
|6/13|Nvidia reaches 1 trillion valuation|
||Nvidia’s market capitalization consistently exceeds 1 trillion USD, driven by rising demand for its AI-powering chips. Nvidia becomes the fifth company to reach a valuation of 1 trillion, joining the ranks of Apple Inc. (AAPL.O), Alphabet Inc. (GOOGL.O), Microsoft Corp. (MSFT.O), and Amazon. com Inc. (AMZN.O).|
|6/26|Databricks buys MosaicML for 1.3 billion|
||Databricks, a leader in data storage and management, announces its acquisition of MosaicML, a generative AI orchestration startup founded in 2021, for 1.3 billion. This move aims to enhance Databricks’ generative AI capabilities.|
|6/29|Thomson Reuters acquires Casetext for 650 million|
||Thomson Reuters finalizes its acquisition of Casetext, a legal startup renowned for its artificial intelligence–powered assistant for law, for a staggering 650 million. At the time of acquisition, Casetext boasted a substantial customer base of over 10,000 law firms and corporate legal departments. Among its flagship offerings is CoCounsel, an AI legal assistant driven by GPT4, which enables rapid document review, legal research memos, deposition preparation, and contract analysis within minutes.|
|6/30|Inflection AI raises 1.3 billion from Bill Gates and Nvidia, among others|
||Inflection AI raises 1.3 billion through a combination of cash and cloud credits, bringing the company’s valuation to over 4 billion. Founded by Mustafa Suleyman of Google DeepMind and Reid Hoffman of LinkedIn, Inflection AI is developing a “kind and supportive” chatbot named Pi. The funding round attracts investments from Microsoft, Nvidia, Reid Hoffman, Bill Gates, and Eric Schmidt, former CEO of Google.|
|8/24|Hugging Face raises 235 million from investors|
||Hugging Face, a platform and community dedicated to machine learning and data science, secures an impressive 235 million funding round, pushing its valuation to 4.5 billion. The platform serves as a one-stop destination for building, deploying, and training machine learning models. Offering a GitHub-like hub for AI code repositories, models, and datasets, Hugging Face has attracted significant attention from industry giants.|
|9/26|SAP introduces new generative AI assistant Joule|
||Joule is a ChatGPT-style digital assistant integrated across SAP’s diverse product range. Joule will seamlessly integrate into SAP applications spanning HR, finance, supply chain, procurement, and customer experience. Additionally, it will be incorporated into the SAP Business Technology Platform, extending its utility across SAP’s extensive user base of nearly 300 million.|
|10/27|Amazon and Google make multibillion-dollar investments in Anthropic|
||Amazon announces its intent to invest up to 4 billion in Anthropic, a rival of OpenAI. This significant investment follows Google’s agreement to invest up to 2 billion in Anthropic. The deal comprises an initial 500 million upfront, with an additional 1.5 billion to be invested over time.|
|11/5|Kai-Fu Lee launches OpenSource Large Language Model|
||Kai-Fu Lee’s Large Language Model startup publicly unveils an open-source model and secures funding at a 1 billion valuation, with Alibaba leading the investment. Lee, known for his leadership roles at Google in China and for establishing Microsoft Research China, one of Microsoft’s key international research hubs, spearheads this initiative.|
|11/17|Sam Altman, OpenAI CEO, fired and then rehired|
||OpenAI’s board claims Altman was “not consistently candid in his communications.” Chaos ensues at OpenAI.Many employees resign in response to the news, and 745 sign a letter threatening resignation if the current board members do not resign. A few days later, Altman is reinstated.|
|12/11|Mistral AI closes 415 million funding round|
||Less than six months after raising a 112 million seed round, Europe-based Mistral AI secures an additional 415 million. The startup, cofounded by alumni from Google’s DeepMind and Meta, focuses on developing foundation models with an open-source technology approach, aiming to compete with OpenAI. Leading the round is Andreessen Horowitz, with participation from Lightspeed Venture Partners, Salesforce, BNP Paribas, General Catalyst, and Elad Gil.|

### 4.2 Jobs

* The analysis of AI labor demand from Lightcast provides valuable insights into the evolving landscape of AI-related skills in the labor market.

|Key|Details|
|-|-|
|Global AI Labor Demand|In 2023, the United States, Spain, and Sweden led in the demand for AI skills, with the United States accounting for 1.6% of AI-related job postings. However, there was a decrease in the share of job postings requiring AI skills in many countries compared to 2022, possibly due to changes in hiring patterns by major AI employers and a shift in the occupational mix of job postings.|
|U.S. AI Labor Demand by Skill Cluster and Specialized Skill|Machine learning remains the most sought-after AI skill in the U.S. labor market, followed by artificial intelligence and natural language processing. While there was a recent dip in the market share of AI-related skill clusters, generative AI saw significant growth. Python's popularity as a programming language for AI also notably increased over the past decade.|
|Top 10 Specialized Skills in AI Job Postings|Over the past decade, there has been an overall increase in the demand for specialized AI skills. Notably, Python has seen a significant rise in popularity. In 2023, there were substantial increases in job postings citing generative AI skills, with specific mentions of large language modeling and ChatGPT.|
|U.S. AI Labor Demand by Sector|Nearly every sector in the U.S. experienced a decrease in the proportion of AI job postings in 2023 compared to 2022, except for public administration and educational services. The leading sectors in AI job postings were information, professional, scientific, and technical services, and finance and insurance.|

* The hiring data presented in the AI Index is based on a LinkedIn dataset of skills and jobs that appear on
their platform.

|Key|Details|
|-|-|
|Hiring| The relative AI hiring rate year-over-year ratio by geographic area indicates the growth in AI talent recruitment compared to overall hiring rates. In 2023, Hong Kong, Singapore, and Luxembourg experienced the most significant rise in AI hiring rates. Starting from 2023, countries like Australia, Canada, Singapore, and India saw a noticeable increase in AI hiring.
|Skill Penetration|AI skill penetration measures the prevalence of AI skills across occupations or the intensity with which LinkedIn members utilize AI skills in their jobs. From 2015 to 2023, India, the United States, and Germany had the highest AI skill penetration rates. Gender-wise, the relative AI skill penetration rate is generally higher for men than women in all countries. India, the United States, and Israel had the highest relative AI skill penetration rates for women.|
|Talent|AI talent concentration measures the portion of LinkedIn members who have explicitly added AI skills to their profile or work in AI. In 2023, Israel, Singapore, and South Korea had the highest concentrations of AI talent. Since 2016, countries like India, Cyprus, and Denmark experienced substantial increases in their AI talent pools.|
|Talent Migration|Net AI talent migration data reveals trends in the movement of AI talent across geographic areas. In recent years, Israel, India, and South Korea have seen declining net AI talent migration figures, indicating an increasing outflow of AI talent from these countries. Luxembourg, Switzerland, and the United Arab Emirates reported the greatest incoming migration of AI talent per 10,000 LinkedIn members.|

### 4.3 Investment

>This section monitors AI investment trends, leveraging data from Quid, which analyzes investment data from more than 8 million companies worldwide, both public and private. Employing natural language processing, Quid sifts through vast unstructured datasets—including news aggregations, blogs, company records, and patent databases—to detect patterns and insights. Additionally, Quid is constantly expanding its database to include more companies, sometimes resulting in higher reported investment volumes for specific years. For the first time, this year’s investment section in the AI Index includes data on generative AI investments.

|Investment|Detail|
|-|-|
|Corporate|Global corporate investment in AI experienced a decline for the second consecutive year in 2023, dropping to $189.2 billion, a 20% decrease from 2022.|
||The most significant downturn occurred in mergers and acquisitions, which fell by 31.2% from the previous year.|
||Despite recent declines, AI-related investments have increased thirteenfold over the past decade.|
|Startup|Private investment trends in AI startups that received over $1.5 million since 2013 are analyzed.|
||While overall private AI investment decreased in 2023, funding for generative AI sharply increased, reaching $25.2 billion, nearly nine times the investment of 2022.|
||The number of newly funded AI companies increased by 40.6% in 2023 compared to the previous year.|
|Regional Comparison|The United States led global private AI investment in 2023, followed by China and the United Kingdom.|
||The gap in private investments between the United States and other regions is widening over time, with significant increases observed in the U.S. since 2022.|
||The U.S. outpaced other regions in generative AI-related investments by a significant margin.|
|Focus Area Analysis|AI infrastructure/research/governance, NLP and customer support, and data management and processing attracted the most investment in 2023.|
||Investments in medical and healthcare, as well as NLP and customer support, peaked in 2021 and have declined since then.|
||Private investment priorities in AI vary across geographies, with the U.S. leading in most focus area categories except for facial recognition, where China leads, and semiconductor investments, where China closely follows the U.S.|

### 4.4 Corporate Activity

>This section examines the practical application of AI by corporations, highlighting industry adoption trends, how businesses are integrating AI, the specific AI technologies deemed most beneficial, and the impact of AI adoption on financial performance.

* This section incorporates insights from McKinsey’s “The State of AI in 2023: Generative AI’s Breakout Year,” alongside data from prior editions.

|Key|Adoption of AI Capabilities|
|-|-|
|Overall AI Adoption Trends|In 2023, 55% of surveyed organizations had implemented AI in at least one business unit or function, showcasing a steady increase from 50% in 2022 and a significant jump from 20% in 2017.|
||McKinsey anticipates even greater changes in AI adoption rates in the future due to rapid technical advancements and increasing adoption.|
|AI Use Cases by Function|Contact-center automation emerged as the most commonly adopted AI use case (26%), followed by personalization, customer acquisition, and AI-based enhancements of products.|
||Different industries exhibited varying levels of adoption for specific AI functions.|
|Embedded AI Capabilities|Robotic process automation had the highest embedding rate within the financial services industry, followed by virtual agents. NL text understanding, robotic process automation, and virtual agents were the most embedded AI technologies across all industries.|
|AI Adoption by Industry and Function|The greatest adoption was observed in product and/or service development for the tech, media, and telecom sector.|
||Significant annual gains in AI adoption were noted in marketing and sales, product/service development, and service operations across all industries.|
|Hiring Trends for AI Positions|Data engineers, AI data scientists, and machine-learning engineers were the most commonly hired positions across industries, with notable demand in the financial services and tech, media, and telecom sectors for machine-learning engineers.|
|Impact of AI on Costs and Revenue|Organizations reported experiencing both cost reductions and revenue increases due to AI adoption, with manufacturing, service operations, and risk management being the top areas for cost savings, and manufacturing, marketing and sales, and risk management being the top areas for revenue gains.|
|Global AI Adoption Trends|Every surveyed region reported higher AI adoption rates in 2023 compared to 2022, with significant year-over-year growth seen in Europe and Greater China, while North America remained the leader in AI adoption.|

|Key|Adoption of Generative AI Capabilities|
|-|-|
|Text Document Generation|The most common application of generative AI is in generating initial drafts of text documents, with 9% of respondents reporting its use.|
|Personalized Marketing|Following closely behind is personalized marketing, with 8% of respondents using generative AI for this purpose.|
|Text Summarization|Another significant application is summarizing text documents, also reported by 8% of respondents.|
|Image and Video Creation|Additionally, 8% of respondents are utilizing generative AI for creating images and/or videos.|
|Function|These applications are predominantly seen within the marketing and sales functions. When compared to the broader usage of AI, generative AI still forms a smaller portion, with general AI dominating overall usage patterns within organizations. However, within specific functions, generative AI is making significant inroads. Notably, its usage is prominent in marketing and sales (14%), product and/or service development (13%), and service operations (10%).|
|Region|North America leads in the adoption of generative AI, with 40% of organizations utilizing it, followed closely by developing markets like India, Latin America, and the MENA region. Stands at 33% globally, significantly lower than the overall adoption rate of AI, which is 55% across all geographies.|

* The integration of AI tools into the workflows of computer developers is not just a trend; it's becoming a cornerstone of professional development. Stack Overflow's 2023 survey, with responses from over 90,000 developers, sheds light on how these professionals are utilizing and perceiving AI tools.

|Professional development|Details|
|-|-|
|Tool Usage| According to the survey, GitHub’s Copilot is the most widely used AI developer tool, with 56.0% of respondents reporting its usage. Following Copilot are Tabnine (11.7%) and AWS CodeWhisperer (4.9%).|
|Search Tools|In the realm of AI search tools, ChatGPT emerges as the clear favorite among professional developers, with 83.3% of respondents utilizing it. Bing AI and WolframAlpha follow with 18.8% and 11.2% usage respectively.|
|Cloud|Cloud platforms play a pivotal role in facilitating computationally intensive AI work. Amazon Web Services (AWS) leads the pack, with 53.1% of developers reporting regular use, trailed by Microsoft Azure (27.8%) and Google Cloud (24.0%).|
|Workflow Integration|A significant majority (82.6%) of developers are integrating AI into their workflows for code writing, with substantial numbers also using it for debugging and assistance (48.9%) and documentation (34.4%). Additionally, there's a growing interest (55.2%) in adopting AI for code testing purposes.|
|Advantages of AI Tools|Developers cite increased productivity (32.8%), accelerated learning (25.2%), and enhanced efficiency (25.0%) as the primary advantages of AI tools in professional development.|
|Sentiments Towards AI Tools|The sentiment towards AI tools among professional developers is overwhelmingly positive. A vast majority (76.1%) express either very favorable or favorable opinions, while only a small minority (3.2%) hold unfavorable views.|
|Trust in AI Tools|When it comes to trust, a significant proportion of developers (42.2%) report high or moderate trust in AI tools' output accuracy. Conversely, 27.2% express some level of distrust or high distrust in these technologies.|

* The integration of AI into various sectors has indeed been a topic of great interest, particularly concerning its impact on productivity and the labor force.

|Key|Details|
|-|-|
|Increased Productivity and Quality|AI tools like Microsoft Copilot, GPT-4, and AI systems used in call centers have been shown to significantly reduce task completion time, improve quality, and enhance efficiency across various industries, including software development, consulting, and customer service. These improvements suggest that AI can augment human capabilities, enabling workers to accomplish tasks more effectively and deliver higher-quality results.|
|Narrowing the Skill Gap|Interestingly, AI access appears to diminish the performance gap between low- and high-skilled workers. While higher-skilled workers still benefit from AI, lower-skilled workers experience larger performance gains when utilizing AI tools. This suggests that AI has the potential to democratize access to productivity-enhancing technologies and mitigate skill-based disparities in the workforce.|
|Potential Pitfalls of Overreliance|While AI can enhance productivity and accuracy, there are risks associated with overreliance on the technology. In the case of professional recruiters, reliance on what is perceived as "good AI" led to decreased performance compared to those using "bad AI." This phenomenon highlights the importance of maintaining a balanced approach to AI integration, where humans remain actively engaged in decision-making processes and critical evaluation of AI outputs.|

* The following section presents data from Quid, which uses natural language processing tools to analyze trends in corporate earnings calls

|Key|Details|
|-|-|
|Aggregate Trends|In 2023, AI was mentioned in 394 earnings calls, which is almost 80% of all Fortune 500 companies. This marks a significant increase from 266 mentions in 2022.|
||Since 2018, mentions of AI in these earnings calls have nearly doubled, indicating a growing interest and integration of AI technologies into corporate strategies.|
|Specific Themes|**Generative AI**: This was the most frequently cited theme, appearing in about 19.7% of all earnings calls. It's noteworthy that mentions of generative AI grew substantially from the previous year,indicating a heightened focus on this area.|
||**Investments in AI, Expansion of AI capabilities, and AI Growth Initiatives**: This theme appeared in 15.2% of earnings calls, suggesting that many companies are actively investing in and expanding their AI capabilities as part of their growth strategies.|
||**Company/Brand AIs**: This theme was mentioned in 7.6% of earnings calls, indicating that some companies are developing AI specifically tailored to their brands or company operations.|

### 4.5 Robot Installations

>The deployment of robots equipped with AI-based software technologies offers a window into the real-world application of AI-ready infrastructure. This section draws on data from the International Federation of Robotics (IFR), a nonprofit organization dedicated to advancing the robotics industry. Annually, the IFR publishes the World Robotics Reports, which track global robot installation trends.

* In 2022, the total number of industrial robots installed globally reached 553,000 units. A 5.1% increase from the installations recorded in 2021, indicating a modest growth trajectory. Over the past decade, there has been significant growth in industrial robot installations, with the number of units installed more than tripling since 2012. These statistics highlight the increasing integration of automation technology, specifically industrial robots, into various industrial processes worldwide. The steady growth in installations underscores the role of automation in enhancing efficiency, productivity, and competitiveness in manufacturing and industrial sectors.

|Robots|Details|
|-|-|
|Industrial||
|Global Installations| In 2022, there were 553,000 industrial robots installed worldwide, marking a 5.1% increase from 2021.The global operational stock reached 3,904,000 in 2022, up from 3,479,000 in 2021.
|Traditional vs. Collaborative|Collaborative robots accounted for 9.9% of all new industrial robot installations in 2022, up from 2.8% in 2017.|
|Geographic Distribution|China led the world with 290,300 industrial robot installations in 2022, followed by Japan, the United States, South Korea, and Germany. Since 2013, China has been the leading installer of industrial robots, with its share rising to 52.4% in 2022.|
|Growth Rates by Country|Countries like Singapore, Turkey, and Mexico reported high growth rates in industrial robot installations, while Canada, Taiwan, Thailand, and Germany reported decreases.|
|Sectors and Applications|Electrical/electronics and automotive sectors led in robot installations, with handling being the predominant application.|
|China|The leading sectors for installations were electrical/electronics, automotive, and metal and machinery in 2022.|
|U.S.|The automotive industry led in industrial robot installations in 2022, with a significant increase compared to 2021.|
|Service Robots||
|Installation Trends|More service robots were installed in various application categories in 2022 compared to 2021, except for medical robotics.|
|Manufacturing|The United States leads in professional service robot manufacturing, followed by China, Germany, Japan, and France.|

## Chapter 5: Science and Medicine

>This year’s AI Index introduces a new chapter on AI in science and medicine in recognition of AI’s growing role in scientific and medical discovery. It explores 2023’s standout AI-facilitated scientific achievements, including advanced weather forecasting systems like GraphCast and improved material discovery algorithms like GNoME. The chapter also examines medical AI system performance, important 2023 AI-driven medical innovations like SynthSR and ImmunoSEIRA, and trends in the approval of FDA AI-related medical devices.

||Chapter Highlights|
|-|-|
|1.|Scientific progress accelerates even further, thanks to AI. In 2022, AI began to advance scientific discovery. 2023, however, saw the launch of even more significant science-related AI applications—from AlphaDev, which makes algorithmic sorting more efficient, to GNoME, which facilitates the process of materials discovery.|
|2.|AI helps medicine take significant strides forward. In 2023, several significant medical systems were launched, including EVEscape, which enhances pandemic prediction, and AlphaMissence, which assists in AI-driven mutation classification. AI is increasingly being utilized to propel medical advancements.|
|3.|Highly knowledgeable medical AI has arrived. Over the past few years, AI systems have shown remarkable improvement on the MedQA benchmark, a key test for assessing AI’s clinical knowledge. The standout model of 2023, GPT-4 Medprompt, reached an accuracy rate of 90.2%, marking a 22.6 percentage point increase from the highest score in 2022. Since the benchmark’s introduction in 2019, AI performance on MedQA has nearly tripled.|
|4.|The FDA approves more and more AI-related medical devices. In 2022, the FDA approved 139 AI-related medical devices, a 12.1% increase from 2021. Since 2012, the number of FDA-approved AI-related medical devices has increased by more than 45-fold. AI is increasingly being used for real-world medical purposes.|

### 5.1 Notable Scientific Milestones

>This section highlights significant AI-related scientific breakthroughs of 2023 as chosen by the AI Index Steering Committee.

||Notable Scientific Milestones|
|-|-|
|AlphaDev discovers faster sorting algorithms|AlphaDev developed algorithms with fewer instructions than existing human benchmarks for fundamental sorting algorithms on short sequences such as Sort 3, Sort 4, and Sort 5. Some of the new algorithms discovered by AlphaDev have been incorporated into the LLVM standard C++ sort library. This marks the first update to this part of the library in over 10 years and is the first addition designed using reinforcement learning.|
|F3D mesh optimization with FlexiCubes|3D mesh optimization with FlexiCubes 3D mesh generation, crucial in computer graphics, involves creating a mesh of vertices, edges, and faces to define 3D objects. It is key to video games, animation, medical imaging, and scientific visualization. FlexiCubes employs AI for gradient-based optimization and adaptable parameters. This method allows for precise, localized mesh adjustments. FlexiCubes achieves mesh extractions that align much more closely with the underlying ground truth.|
|Synbot, AI-driven robotic chemist for synthesizing organic molecules|AI-driven robotic chemist for synthesizing organic molecules Synbot employs a multilayered system,comprising an AI software layer for chemical synthesis planning, a robot software layer for translating commands, and a physical robot layer for conducting experiments. The closed-loop feedback mechanism between the AI and the robotic system enables Synbot to develop synthetic recipes with yields equal to or exceeding established references. Synbot’s automation of organic synthesis highlights AI’s potential in fields such as pharmaceuticals and materials science.|
|More accurate global weather forecasting with GraphCast|GraphCast is a new weather forecasting system that delivers highly accurate 10-day weather predictions in under a minute. Utilizing graph neural networks and machine learning, GraphCast processes vast datasets to forecast temperature, wind speed, atmospheric conditions, and more. Compares the performance of GraphCast with the current industry state-of-theart weather simulation system: the High Resolution Forecast (HRES). GraphCast posts a lower root mean squared error, meaning its forecasts more closely correspond to observed weather patterns. GraphCast can be a valuable tool in deciphering weather patterns, enhancing preparedness for extreme weather events, and contributing to global climate research.|
|Discovering new materials with GNoME|Google researchers have demonstrated that graph networks, a type of AI model, can expedite this process when trained on large datasets. GNoME, outperformed the Materials Project, a leading method in materials discovery, by identifying a significantly larger number of stable. The success of AI-driven projects like GNoME highlights the power of data and scaling in speeding up scientific breakthroughs.|
|Flood Forecasting, AI for more accurate and reliable flood forecasts|A team of Google researchers has used AI to develop highly accurate hydrological simulation models that are also applicable to ungauged basins. These innovative methods can predict certain extreme flood events up to five days in advance, with accuracy that matches or surpasses current state-of-the-art models, such as GloFAS. The AI model demonstrates superior precision (accuracy of positive predictions) and recall (ability to correctly identify all relevant instances) across a range of return period events, outperforming the leading contemporary method. It is open-source and is already being used to predict flood events in over 80 countries.|

### 5.2 AI in Medicine

> AI models are becoming increasingly valuable in healthcare, with applications for detecting polyps to aiding clinicians in making diagnoses. As AI performance continues to improve, monitoring its impact on medical practice becomes increasingly important. This section highlights significant AI-related medical systems introduced in 2023, the current state of clinical AI knowledge, and the development of new AI diagnostic tools and models aimed at enhancing hospital administration.

||Notable Medical Systems|
|-|-|
|SynthSR, Transforming brain scans for advanced analysis|SynthSR is an AI tool that converts clinical brain scans into high-resolution T-1 weighted images. This advancement addresses the issue of scan quality variability, which previously limited the use of many scans in advanced research. By transforming these scans into T1-weighted images, known for their high contrast and clear brain structure depiction, SynthSR facilitates the creation of detailed 3D brain renderings. It significantly improves the visualization and analysis of brain structures, facilitating neuroscientific research and clinical diagnostics.|
|Coupled plasmonic infrared sensors for the detection of neurodegenerative diseases|Diagnosis of neurodegenerative diseases such as Parkinson’s and Alzheimer’s depends on fast andprecise identification of biomarkers.This year, researchers uncovered a new method for neurodegenerative disease diagnosis that combined AI-coupled plasmonic infrared sensors that use Surface-Enhanced Infrared Absorption (SEIRA) spectroscopy with an immunoassay technique. In tests that compared actual fibril percentages with predictions made by AI systems, the accuracy of the predictions was found to very closely match the actual reported percentages.|
|EVEscape, Forecasting viral evolution for pandemic preparedness|EVEscape is a new AI deep learning model trained on historical sequences and biophysical and structural information that predicts the evolution of viruses. EVEscape evaluates viral escape independently of current strain data predicting 50.0% of observed SARS-CoV-2 mutations, outperforming traditional lab studies which predicted 46.2% and 32.3%, as well as a previous model, which predicted only 24% of mutations.|
|AlphaMissence, Better classification of AI mutations|In 2023, researchers from Google DeepMind unveiled a new AI model that predicted the pathogenicity of 71 million missense variants. Missense mutations are genetic alterations that impact the functionality of human proteins and can lead to various diseases, including cancer.|
|**Human Pangenome Reference, Using AI to map the human genome**|In 2023, the Human Pangenome Research Consortium, comprising 119 scientists from 60 institutions, used AI to develop an updated and more representative human genome map. The researchers achieved remarkable accuracy, annotating a median of 99.07% of protein-coding genes, 99.42% of protein-coding transcripts, 98.16% of noncoding genes, and 98.96% of noncoding transcripts.|

||Clinical Knowledge|
|-|-|
|MedQA|Introduced in 2020, MedQA is a comprehensive dataset derived from professional medical board exams, featuring over 60,000 clinical questions designed to challenge doctors. AI performance on the MedQA benchmark has seen remarkable improvement, with the leading system, GPT-4 Medprompt, achieving an accuracy rate of 90.2%—an increase of 22.6 percentage points from the top score in 2022.|
|GPT-4 Medprompt|Fine-tuning entails training a Large Language Model on domain-specific data. Research from Microsoft in late 2023 has overturned this assumption. This study employed prompt engineering to direct GPT-4 toward achieving remarkable performance on the MultiMedQA benchmark suite, a group of four challenging medical benchmarks. PT-4 Medprompt exceeded the performance of the top 2022 model, Flan-PaLM 540B, in the multiplechoice sections of several renowned medical benchmarks, including PubMedQA, MedMCQA, and MMLU, by 3.0, 21.5, and 16.2 percentage points, respectively. It also exceeded the performance of the then state-of-the-art Med-PaLM 2.Moreover, as noted earlier, GPT-4 Medprompt was the first to surpass the 90% accuracy mark on the MedQA benchmark. This breakthrough not only underscores GPT-4 Medprompt’s exceptional and potentially clinically useful medical capabilities but also demonstrates that fine-tuning may not always be necessary for adapting models to specialized domains. Prompt engineering has shown to be a promising alternative strategy,|
|MediTron-70B|GPT-4 Medprompt is an impressive system; however, it is closed-source, meaning its weights are not freely available to the broader public for use. New research in 2023 has also sought to advance the capabilities of open-source medical Large Language Models. Among this new research, MediTron-70B stands out as particularly promising. This model achieves a respectable 70.2% accuracy on the MedQA benchmark. Although this is below the performance of GPT-4 Medprompt and MedPaLM 2 (both closed models), it represents a significant improvement over the state-ofthe-art results from 2023 and surpasses other open-source models like Llama 2.MediTron-70B’s score on MedQA is the highest yet achieved by an open-source model. If medical AI is to reach its fullest potential, it is important that its capabilities are widely accessible. In this context, MediTron represents an encouraging step forward.|

||Diagnosis|
|-|-|
|CoDoC|In 2023, researchers unveiled CoDoC (Complementarity-Driven Deferral to Clinical Workflow), a system designed to discern when to rely on AI for diagnosis and when to defer to traditional clinical methods. CoDoC notably enhances both sensitivity (the ability to correctly identify individuals with a disease) and specificity(the ability to accurately identify those without it). These findings suggest that AI medical systems can be integrated into clinical workflows, thereby enhancing diagnostic accuracy and efficiency.|
|CT Panda, Pancreatic ductal adenocarcinoma (PDAC) is a particularly lethal cancer, often detected too late for surgical intervention.|This year, a Chinese research team developed PANDA (pancreatic cancer detection with artificial intelligence), an AI model capable of efficiently detecting and classifying pancreatic lesions in X-rays.PANDA surpassed the average radiologist in sensitivity by 34.1% and in specificity by 6.3%. In a large-scale, real-world test involving approximately 20,000 patients, PANDA achieved a sensitivity of 92.9% and a specificity of 99.9%. PANDA represent significant advancements in diagnosing challenging conditions, offering cost-effective and accurate detection previously considered difficult or prohibitive.|
|The U.S. Food and Drug Administration (FDA) maintains a list of AI/ML-enabled medical devices that have received approval. As of October 2023, the FDA has not approved any devices that utilize generative AI or are powered by Large Language Models.|In 2022, a total of 139 AI-related medical devices received FDA approval, marking a 12.1% increase from the total approved in 2021. Since 2012, the number of these devices has increased by more than 45-fold. Of the 139 devices approved in 2022, a significant majority, 87.1%, were related to radiology. The next most common specialty wascardiovascular, accounting for 7.2% of the approvals.|

||Administration and Care|
|-|-|
|MedAlign|This year researchers have made a comprehensive EHR-based benchmark with 983 questions and instructions and 303 clinician responses, drawn from seven different medical specialties. MedAlign is the first extensive EHR-focused benchmark. The researchers then tested various existing Large Language Models on MedAlign. Of all Large Language Models, a GPT-4 variant using multistep refinement achieved the highest correctness rate (65.0%) and was routinely preferred over other Large Language Models. MedAlign is a valuable milestone toward using AI to alleviate administrative burdens in healthcare.|

## Chapter 6: Education

>This chapter examines trends in AI and computer science (CS) education, focusing on who is learning, where they are learning, and how these trends have evolved over time. Amid growing concerns about AI’s impact on education, it also investigates the use of new AI tools like ChatGPT by teachers and students.
The analysis begins with an overview of the state of postsecondary CS and AI education in the United States and Canada, based on the Computing Research Association’s annual Taulbee Survey. It then reviews data from Informatics Europe regarding CS education in Europe. This year introduces a new section with data from Studyportals on the global count of AI-related English-language study programs.
The chapter wraps up with insights into K–12 CS education in the United States from Code.org and findings from the Walton Foundation survey on ChatGPT’s use in schools.


||Chapter Highlights|
|-|-|
|1.|The number of American and Canadian CS bachelor’s graduates continues to rise, new CS master’s graduates stay relatively flat, and PhD graduates modestly grow. While the number of new American and Canadian bachelor’s graduates has consistently risen for more than a decade, the number of students opting for graduate education in CS has flattened. Since 2018, the number of CS master’s and PhD graduates has slightly declined.|
|2.|The migration of AI PhDs to industry continues at an accelerating pace. In 2011, roughly equal percentages of new AI PhDs took jobs in industry (40.9%) and academia (41.6%). However, by 2022, a significantly larger proportion (70.7%) joined industry after graduation compared to those entering academia (20.0%). Over the past year alone, the share of industry-bound AI PhDs has risen by 5.3 percentage points,indicating an intensifying brain drain from universities into industry.|
|3.|Less transition of academic talent from industry to academia. In 2019, 13% of new AI faculty in the United States and Canada were from industry. By 2021, this figure had declined to 11%, and in 2022, it further dropped to 7%. This trend indicates a progressively lower migration of high-level AI talent from industry into academia.|
|4.|CS education in the United States and Canada becomes less international. Proportionally fewer international CS bachelor’s, master’s, and PhDs graduated in 2022 than in 2021. The drop in international students in the master’s category was especially pronounced.|
|5.|More American high school students take CS courses, but access problems remain. In 2022, 201,000 AP CS exams were administered. Since 2007, the number of students taking these exams has increased more than tenfold. However, recent evidence indicates that students in larger high schools and those in suburban areas are more likely to have access to CS courses.|
|6.|AI-related degree programs are on the rise internationally. The number of English-language, AI-related postsecondary degree programs has tripled since 2017, showing a steady annual increase over the past five years. Universities worldwide are offering more AI-focused degree programs.|
|7.|The United Kingdom and Germany lead in European informatics, CS, CE, and IT graduate production. The United Kingdom and Germany lead Europe in producing the highest number of new informatics, CS, CE, and information bachelor’s, master’s, and PhD graduates. On a per capita basis, Finland leads in the production of both bachelor’s and PhD graduates, while Ireland leads in the production of master’s graduates.|

### 6.1 Postsecondary CS and AI Education

>This section provides an overview of postsecondary education in CS and AI, highlighting graduation statistics across North America and Europe for various degrees including bachelor’s, master’s, and PhDs. It also covers information on AI-related courses offered in English.

|U.S.|Postsecondary CS and AI Education|
|-|-|
|CS Bachelor's Graduates|The number of CS bachelor's graduates in North America has steadily increased over the past decade, with a significant rise in recent years.There was a slight decline in the proportion of international students among CS bachelor's graduates in 2022, which could be attributed to various factors such as visa difficulties and travel restrictions during the pandemic.|
|CS Master's Graduates|While the total number of new CS master's graduates doubled over the past decade, it seems to have leveled out since 2018 and slightly decreased in 2022.There was a notable decrease in international CS master's students in 2022, with the trend starting around 2017.|
|CS PhD Graduates|There was a significant increase in the number of new CS PhD graduates in 2022, reaching the highest level since 2010.Although the proportion of international students among CS PhD graduates has risen over the past decade, there was a slight decrease in 2022.|
|Career Paths of AI PhD Graduates|A growing proportion of AI doctoral recipients are choosing careers in industry over academia, with the majority entering industry roles after graduation.|
CS, CE, and Information Faculty|The total number of CS, CE, and information faculty in American and Canadian universities has been steadily increasing.There has been a notable increase in new faculty appointments, with a significant proportion coming from other academic positions.The competition for new CS faculty positions appears to be increasing, as indicated by a higher rate of declined job offers. International hires among new tenure-track faculty in CS, CE, and information disciplines increased significantly in 2022.|
|Salaries and Departures|Salaries for full professors have not kept pace with inflation rates, while those for assistant and associate professors have seen slight increases.There has been a significant increase in faculty departures, with a notable proportion transitioning to other academic positions.|

|Europe|Postsecondary CS and AI Education|
|-|-|
|Bachelor's Graduates|In 2022, the United Kingdom had the highest number of new graduates in informatics, CS, CE, and IT at the bachelor’s level, with around 25,000 graduates. Germany and Turkey closely followed. Finland, Norway, and the Netherlands lead in the number of new bachelor’s graduates per 100,000 inhabitants.|
|Master's Graduates|The United Kingdom led in producing new master’s graduates in informatics, CS, CE, and IT, with approximately 20,000 graduates. Germany, Turkey, and Spain have seen the greatest percentage growth in new master’s graduates over the last decade. Ireland had the most master’s graduates per capita, followed by the United Kingdom and Estonia.|
|PhD Graduates|The United Kingdom and Germany produced the most informatics, CS, CE, and IT PhD graduates in 2022, followed by Italy.Turkey has seen the greatest growth in new CS, CE, and IT PhD graduates over the last decade. Finland leads in the number of new informatics, CS, CE, and IT PhD graduates per capita.|

||AI-Related Study Programs|
|-|-|
|Total Courses|The number of English-language AI-related study programs has tripled since 2017, indicating a consistent yearly increase over the last five years. This suggests a growing educational interest in AI.|
|Education Level|The majority of AI study programs are offered at the master’s level (55.0%), followed by the bachelor’s level (39.8%), and finally at the PhD level (5.3%). This distribution reflects the varying depths of study and specialization within AI education.|
|Geographic Distribution|In 2023, the United Kingdom had the greatest number of English-language AI study programs (744), followed by the United States (667) and Canada (89). Almost every country included in the sample experienced an increase in the number of AI university study programs in 2023 compared to 2022. Notably, Malta, the United Kingdom, and Cyprus had the highest number of AI university study programs per capita in 2023.|

### 6.2 K–12 CS and AI Education

>This section presents trends in high school CS education in the United States as a representation of K–12 AI education.

||K–12 CS and AI Education|
|-|-|
|High School CS Course Requirement|As of 2023, 30 states in the US mandated that all high schools offer a foundational CS course.|
|Variation in CS Course Offerings|The percentage of public schools offering CS courses varies greatly from state to state. Maryland, Arkansas, and Nevada are the top three states with the highest percentage of CS offerings, while Minnesota, Montana, and Louisiana have the lowest.|
|Expansion of K–12 CS Education| From 2017 to the present, there has been significant expansion in K–12 CS education across the country. Approximately two-thirds of states now require CS to be taught in high schools, allocate funding for it, and have developed state plans for CS education.|
|AP Computer Science Exams|The number of AP CS exams administered has been increasing steadily. In 2022,approximately 201,000 exams were administered, marking an 11.1% increase from 2021. California, Texas, and Florida led in the number of AP CS exams taken, while Montana, South Dakota, and North Dakota had the fewest.Per capita, Maryland, New Jersey, and Massachusetts ranked highest in the number of AP CS exams taken.|
|Access to CS Education| According to Code.org data, factors such as school size and location significantly influence the accessibility of computer science (CS) education. Large schools (over 1,200 students) are 15 percentage points more likely to offer CS courses compared to medium-sized schools (500–1,200 students), and the gap widens further when compared to small schools (under 500 students). Similarly, students in suburban districts have better access to CS courses compared to their counterparts in both urban and rural areas.|
|ChatGPT Usage Among Teachers and Students|Impact Research conducted surveys on American teachers’ and educators’ perceptions and use of ChatGPT, revealing a significant adoption rate. The surveys indicate that a majority of K–12 teachers in the United States are already utilizing ChatGPT, with usage increasing over time. In March 2023, 51% of teachers reported having used ChatGPT at least once, rising to 63% by July 2023.Among teachers who reported using ChatGPT, 30% employed it for lesson planning, 30% for generating new creative class ideas, and 27% for enhancing their background knowledge. Both teachers and students have overwhelmingly positive attitudes toward ChatGPT. In March 2023, 88% of teachers believed that ChatGPT had a positive impact, a sentiment echoed by 79% of the students surveyed. Furthermore, 76% of teachers and 65% of students feel that ChatGPT is important to incorporate into the educational process.|

## Chapter 7: Policy and Governance

>AI’s increasing capabilities have captured policymakers’ attention. Over the past year, several nations and political bodies, such as the United States and the European Union, have enacted significant AI-related policies. The proliferation of these policies reflect policymakers’ growing awareness of the need to regulate AI and improve their respective countries’ ability to capitalize on its transformative potential.
This chapter begins examining global AI governance starting with a timeline of significant AI policymaking events in 2023. It then analyzes global and U.S. AI legislative efforts, studies AI legislative mentions, and explores how lawmakers across the globe perceive and discuss AI. Next, the chapter profiles national AI strategies and regulatory efforts in the United States and the European Union. Finally, it concludes with a study of public investment in AI within the United States.

||Chapter Highlights|
|-|-|
|1.|The number of AI regulations in the United States sharply increases. The number of AI-related regulations in the U.S. has risen significantly in the past year and over the last five years. In 2023, there were 25 AI-related regulations, up from just one in 2016. Last year alone, the total number of AI-related regulations grew by 56.3%.|
|2.|The United States and the European Union advance landmark AI policy action. In 2023, policymakers on both sides of the Atlantic put forth substantial AI regulatory proposals. The European Union reached a deal on the terms of the AI Act, a landmark piece of legislation enacted in 2024. Meanwhile, President Biden signed an Executive Order on AI, the most notable AI policy initiative in the United States that year.|
|3.|AI captures U.S. policymaker attention. The year 2023 witnessed a remarkable increase in AI-related legislation at the federal level, with 181 bills proposed, more than double the 88 proposed in 2022.|
|4.|Policymakers across the globe cannot stop talking about AI. Mentions of AI in legislative proceedings across the globe have nearly doubled, rising from 1,247 in 2022 to 2,175 in 2023. AI was mentioned in the legislative proceedings of 49 countries in 2023. Moreover, at least one country from every continent discussed AI in 2023, underscoring the truly global reach of AI policy discourse.|
|5.|More regulatory agencies turn their attention toward AI. The number of U.S. regulatory agencies issuing AI regulations increased to 21 in 2023 from 17 in 2022, indicating a growing concern over AI regulation among a broader array of American regulatory bodies. Some of the new regulatory agencies that enacted AIrelated regulations for the first time in 2023 include the Department of Transportation, the Department of Energy, and the Occupational Safety and Health Administration.|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

### 7.1 Overview of AI Policy in 2023

|2023|Event|Details|
|-|-|-|
|1/10|China introduces regulation on administration of deep synthesis of the internet|China introduces regulations aimed at “deep synthesis” technology to tackle security issues related to the creation of realistic virtual entities and multimodal media, including “deepfakes.” These regulations apply to both providers and users across different media and mandate measures, such as preventing illegal content, adhering to legal compliance, verifying user identities, securing consent for biometric editing, safeguarding data security, and enforcing content moderation.|
|3/22|U.S. legislators propose AI for National Security Act|This legislation clarifies and solidifies the Department ofDefense’s (DoD) authority to acquire AI-based endpoint security tools, enhancing its cyber-defense capabilities. It aims to enable the DoD to employ AI for the automatic detection and mitigation of threats to its networks and digital infrastructure. This bipartisan initiative ensures the DoD can adopt innovative commercial technologies to strengthen its cyber defenses, matching the pace of adversaries.|
|5/11|U.S. policymakers introduce AI Leadership Training Act|This legislation aims to enhance AI literacy among federal leaders in response to AI’s widespread adoption across government agencies. It mandates the director of the Office of Personnel Management (OPM) to create and periodically refresh an AI training program, promoting responsible and ethical AI usage within the federal government. Building on previous laws, the initiative expands AI training to include federal employees involved in procuring AI technologies for government use.|
|6/20|U.S. policymakers propose National AI Commission Act|The National AI Commission Act calls for establishing a National AI Commission tasked with crafting a comprehensive AI regulatory framework. Highlighting the importance of expert input due to AI’s rapid innovation and complexity, this bipartisan initiative focuses on mitigating risks, preserving U.S. leadership in AI research and development, and ensuring consistency with American values.|
|7/6|House of Representatives advances Jobs of the Future Act|The bill endorses a study to evaluate industries and occupations anticipated to grow due to AI, assess its effects on workers’ skills or potential replacement, examine stakeholder influence opportunities, identify the demographics most impacted, evaluate the required skills and education, review data accessibility, investigate efficient skill delivery methods, and explore the role of academic institutions in offering critical training.|
|7/19|U.S. Senate puts forward Artificial Intelligence and Biosecurity Risk Assessment Act|The act mandates the assistant secretary for preparedness and response to assess and address threats to public health and national security from technical advancements in artificial intelligence. It emphasizes evaluating the potential use of AI, including open-source models, for developing harmful agents. The proposed initiatives include monitoring global biological risks and integrating risk assessment summaries into the National Health Security Strategy.|
|7/21|Private AI labs sign voluntary White House AI commitments|The Biden-Harris administration obtains voluntary pledges from seven major AI firms—Google, Microsoft, Meta, Amazon, OpenAI, Anthropic, and Inflection—to promote the development of AI that is safe, secure, and reliable. These commitments involve conducting internal and external security assessments of AI systems prior to launch, sharing information on identified risks, enabling public reporting of issues, and disclosing when content is AI-generated.|
|7/26|U.S. Senate passes Outbound Investment Transparency Act|This initiative aims to scrutinize U.S. investments in critical sectors, especially those involving China, with a focus on evaluating risks in crucial industries and technologies such as AI that impact national security. The objective is to increase awareness of potential vulnerabilities and risks linked to foreign access to American technology in these domains.|
|7/27|U.S. Senate proposes CREATE AI Act|The CREATE AI Act establishes the National Artificial Intelligence Research Resource (NAIRR), a national research infrastructure to improve AI researchers’ and students’ access to essential resources. NAIRR offers compute, curated datasets, educational tools, and AI testbeds. It aims to bolster the nation’s AI research capabilities by supporting the testing and evaluation of AI systems.|
|8/6|China updates cyberspace administration of generative AI measures|China’s updated policy adopts a more targeted regulatory approach, focusing on applications with public implications rather than a blanket regulation. The amendments soften the regulatory language, changing directives like “ensure the truth, accuracy, objectivity, and diversity of the data” to “employ effective measures to enhance the quality of training data and improve its truth, accuracy, objectivity, and diversity.” Additionally, the revised regulations encourage generative AI development, shifting away from the prior punitive focus.|
|9/12|U.S. Senate puts forward Protect Elections from Deceptive AI Act|The bipartisan bill seeks to prohibit the use of AI to create materially deceptive content that falsely represents federal candidates in political advertisements. This act addresses the risks of AI-driven disinformation in elections by banning the distribution of materially deceptive AIgenerated audio or visual content related to candidates running for federal office.|
|9/18|U.K. proposes principles to guide competitive AI markets and protect consumers|The U.K.’s Competition and Markets Authority proposes principles to foster competitive AI markets while ensuring consumer protection. These principles are designed to guarantee accountability for AI outputs, maintain continuous access to essential inputs, promote a diversity of business models, provide businesses with choices, offer flexibility to switch between models, and ensure fair practices to prevent anticompetitive behavior.|
|10/30|President Biden issues Executive Order on Safe, Secure, and Trustworthy AI|The executive order establishes new benchmarks for AI safety, security, privacy protection for Americans, advancement of equity and civil rights, and the fostering of competition and innovation. It mandates the creation of a national security memorandum to guide the safe and ethical application of AI in military and intelligence operations, ensuring the protection of Americans’ privacy and the cultivation of an open, competitive AI market that emphasizes U.S. innovation. Additionally, the Department of Education is tasked with addressing AI’s safe and responsible use in education, while the Federal Communications Commission is encouraged to assess AI’s impact on telecommunications. The National Institute of Standards and Technology (NIST) is instructed to formulate guidelines and best practices to support industry consensus on developing and deploying secure, reliable, and ethical AI.|
|10/30|Frontier AI taskforce releases second progress report|The task force forms new alliances with leading AI organizations and facilitates the development of the U.K.’s AI Research Resource (AIRR), to be known as IsambardAI, an AI supercomputer designed for compute-intensive safety research. Moreover, the report highlights the task force’s initiatives to mitigate risks inherent in advanced AI development and its partnerships with premier AI companies to gain early access to their models.|
|11/1|U.K. hosts AI Safety Summit (2023)|The UK AI Safety Summit at Bletchley Park seeks to tackle AI risks and promote global cooperation, culminating in the Bletchley Declaration. This declaration, endorsed by 28 countries, including China and the United States, signifies a significant global agreement on AI safety. The U.K. also unveiled the world’s inaugural AI Safety Institute, dedicated to safety assessments and research. Despite these developments, reactions are mixed, with certain experts advocating for more comprehensive and ambitious policy measures.|
|11/2|U.K. announces AI Safety Institute|The AI Safety Institute, the first government-supported entity dedicated to advancing AI safety in the public interest, aims to safeguard the U.K. and humanity from unforeseen AI advancements. Its goal is to build the sociotechnical framework required to comprehend and govern the risks associated with advanced AI. By conducting fundamental AI safety research, the institute intends to enhance worldwide comprehension of the dangers posed by advanced AI systems and create the technical tools vital for effective AI governance. Furthermore, it aspires to position the U.K. as a global center for safety research,thereby reinforcing the nation’s strategic investment in this critical technology.|
|12/9|Europeans reach deal on EU AI Act|European lawmakers reach a tentative deal on the AI Act. The act establishes a risk-based regulatory framework for AI, prohibiting systems with unacceptable risks, such as behavioral manipulators, and classifying high-risk systems into product-based and critical sectors. Generative AI, such as ChatGPT, is required to adhere to transparency standards. Meanwhile, low-risk AI, including deepfake technologies, is subject to fundamental transparency obligations.|

### 7.2 AI and Policymaking

||The AI Index analyzed legislation containing “artificial intelligence” in 128 countries from 2016 to 2023.2|
|-|-|
|Geographic Distribution|128 countries were analyzed, with 32 countries enacting at least one AI-related bill. Belgium led in 2023 with five laws, followed by France, South Korea, and the United Kingdom, each passing three.|
|Total Bills Passed|A total of 148 AI-related bills were passed globally during the period analyzed. The United States (23), Portugal (15), and Belgium (12) were the top three countries in terms of the number of AI-related laws passed since 2016.|
|Relevance|Bills were categorized based on their relevance to AI: high, medium, or low. High-relevance bills are fundamentally focused on AI-related policy, while medium-relevance bills incorporate significant AI policy elements but have a broader focus. Low-relevance bills mention AI in passing without substantial focus on AI-related matters.|
|Approach|Bills were classified as expansive (aiming to enhance AI capabilities) or restrictive (imposing limitations on AI usage). There is a global trend towards regulating AI usage, with a growing shift towards restrictive legislation, indicating a focus on mitigating potential AI-related harms.|
|Subject Matter|AI-related bills were classified based on primary subject matter. Historically, economics and public finance were predominant, but in 2023, the distribution broadened significantly, spanning areas such as armed forces and national security, civil rights and liberties, commerce, education, labor and employment, and science, technology, and communication.|

||U.S. Legislative Records|
|-|-|
|Federal Level|The total number of proposed AI-related bills in the U.S. Congress saw a remarkable increase from 88 in 2022 to 181 in 2023. This surge in proposed legislation likely reflects policymakers' response to the growing public awareness and advancements in AI technologies. The data suggests a heightened federal legislative activity aimed at addressing various aspects of AI governance and regulation.|
|State Level|California leads in AI-related legislative activity at the state level, with seven laws enacted in 2023, followed by Virginia with five and Maryland with three. Over the period from 2016 to 2023, California has been consistently proactive in passing AI-related legislation, with a total of 13 bills enacted. The total number of state-level AI-related bills proposed in 2023 reached 150, showing a significant increase from the previous year's count of 61. Importantly, a larger proportion of AI-related bills are enacted into law at the state level compared to the federal level, indicating the states' proactive approach to AI governance and regulation.|

||AI Mentions|
|-|-|
|Global Trends|The mentions of AI in legislative proceedings globally nearly doubled from 1,247 in 2022 to 2,175 in 2023, indicating a significant increase in policymakers' attention to AI-related matters. Since 2016, AI mentions in legislative discussions have risen almost tenfold, suggesting a growing recognition of the importance of AI technologies. In 2023, the United Kingdom led in AI mentions within its legislative proceedings (405), followed by the United States (240) and Australia (227). AI discussions reached legislative platforms in at least one country from every continent in 2023, highlighting the global nature of AI policy discourse.|
|Aggregate Mentions|When aggregated from 2016 to 2023, the United Kingdom had the highest number of AI mentions (1,490), followed by Spain (886) and the United States (868).|
|U.S. Committee Mentions|Mentions of AI in committee reports by House and Senate committees in the United States serve as indicators of legislative interest in AI.While mentions of AI have decreased for the current 118th session, it is noteworthy that this session is only halfway through and is expected to surpass all previous sessions in terms of AI mentions if the current rate continues. The Appropriations and Science, Space, and Technology committees in the House of Representatives feature the highest number of AI mentions during the ongoing 118th congressional session. In the Senate, the Appropriations committee leads in AI mentions, followed by the Homeland Security and Governmental Affairs Committee.|
|Historical Trends|Figures depicting the total number of mentions in committee reports from congressional sessions since 2001 show that the House and Senate Appropriations committees lead their respective lists, reflecting their significant roles in regulating government expenditures.|

### 7.3 National AI Strategies

>This section offers an overview of national AI strategies, which are policy plans created by governments to guide the development and deployment of AI within their country. Monitoring trends in these strategies is important for assessing how countries prioritize the development and regulation of AI technologies. Sources include national or regional government websites, the OECD AI Policy Observatory (oecd.ai), and news reports.

* Canada initiated the first national AI strategy in March 2017. To date, 75 national AI strategies have been unveiled. The peak year was 2019, when 24 strategies were released. In 2023, eight new strategies were added, from countries in the Middle East, Africa, and the Caribbean, showcasing the worldwide expansion of AI policymaking discourse.

### 7.4 AI Regulation

> The advent of AI has garnered significant attention from regulatory agencies—federal bodies tasked with regulating sectors of the economy and steering the enforcement of laws. This section examines AI regulations within the United States and the European Union. Unlike legislation, which establishes legal frameworks within nations, regulations are detailed directives crafted by executive authorities to enforce legislation. In the United States, prominent regulatory agencies include the Environmental Protection Agency (EPA), Food and Drug Administration (FDA), and Federal Communications Commission (FCC). Since the specifics of legislation often manifest through regulatory actions, understanding the AI regulatory landscape is essential in order to develop a deeper understanding of AI policymaking.

|U.S.|AI regulations|
|-|-|
|Overview|The number of AI-related regulations has significantly increased over the past years, with 25 regulations in 2023 compared to just one in 2016. There was a 56.3% increase in AI-related regulations from the previous year.|
|By Relevance| Regulations were categorized into low, medium, and high relevance. Examples include Copyright Registration Guidance for works containing AI-generated material and Cybersecurity Risk Management Strategy by the SEC.|
|By Agency|Primary sources of AI regulations include the Executive Office of the President, Commerce Department, Health and Human Services Department, and Industry and Security Bureau. The number of agencies issuing AI regulations increased from 17 in 2022 to 21 in 2023.|
|By Approach|Regulations were categorized based on whether they expanded or restricted AI capabilities. There's been a shift towards more restrictive regulations over time.|
|By Subject Matter|AI regulations in the U.S. covered various subjects, with foreign trade and international finance being the most prevalent, followed by health, commerce, and science, technology, and communications.|

|Europe|AI regulations|
|-|-|
|Overview|The number of AI-related regulations passed by the European Union increased from 22 in 2022 to 32 in 2023, with a peak of 46 in 2021.|
|By Relevance|In 2023, two regulations had high relevance, 13 had medium relevance, and 17 had low relevance.
|By Agency|The Council of the European Union and the European Parliament were the top originator agencies for AI regulations in 2023.|
|By Approach|European Union AI regulations tended to take a more expansive approach in recent years, with 12 regulations being expansive and 8 restrictive in 2023.|
|By Subject Matter|Common subject matters for AI-related regulations in the EU included science, technology, and communications, as well as government operations and politics.|

### 7.5 U.S. Public Investment in AI

>This section examines public AI investment in the United States based on data from the U.S. government and Govini, a company that uses AI and machine learning technologies to track U.S. public and commercial spending.

||Federal Budget for AI R&D|
|-|-|
|Total|The funding has more than tripled since FY 2018.|
||FY 2023: $1.8 billion|
||FY 2024 (requested): $1.9 billion|
||Breakdown of NITRD AI R&D Budget Requests by Agency for FY 2024:|
||National Science Foundation (NSF): $531 million|
||Defense Advanced Research Projects Agency (DARPA): $322.1 million|
||National Institutes of Health (NIH): $284.5 million|
|Department of Defense (DoD) AI R&D Budget|FY 2023: $1.1 billion requested|
||FY 2024: $1.8 billion requested (a significant increase)|
|AI-Related Contract Spending|Since 2018, total spending has increased nearly 2.4 times.|
||2022: $3.2 billion|
||2023: $3.3 billion|
||AI Segments with the Greatest Government Spending in 2023:|
||Machine Learning: $1.5 billion|
||Computer Vision: $1.0 billion|
||Spending on natural language processing also increased.|
||Types of Federal AI Contracts in 2023:|
||Prime Contracts: 50.6%|
||Grants: 47.6%|
||The share of grants has increased, while the share of contracts has declined.|
|Microelectronics and Semiconductor Spending|Total Spending in 2023: $3.9 billion (up from $2.5 billion in 2022)|
||The majority of spending is allocated as contracts, reflecting the increasing geopolitical significance of semiconductors in AI advancements.|
|Summary|The U.S. government has been significantly increasing its investment in AI R&D, with a focus on both fundamental research through agencies like NSF, DARPA, and NIH, and practical applications via the DoD.Additionally, there is substantial federal spending on AI-related contracts, particularly in machine learning and computer vision. The rise in microelectronics and semiconductor spending highlights the critical role these components play in AI technologies.|

## Chapter 8: Diversity

>The demographics of AI developers often differ from those of users. For instance, a considerable number of prominent AI companies and the datasets utilized for model training originate from Western nations, thereby reflecting Western perspectives. The lack of diversity can perpetuate or even exacerbate societal inequalities and biases.
This chapter delves into diversity trends in AI. The chapter begins by drawing on data from the Computing Research Association (CRA) to provide insights into the state of diversity in American and Canadian computer science (CS) departments. A notable addition to this year’s analysis is data sourced from Informatics Europe, which sheds light on diversity trends within European CS education. Next, the chapter examines participation rates at the Women in Machine Learning (WiML) workshop held annually at NeurIPS. Finally, the chapter analyzes
data from Code.org, offering insights into the current state of diversity in secondary CS education across the United States.
The AI Index is dedicated to enhancing the coverage of data shared in this chapter. Demographic data regarding AI trends, particularly in areas such as sexual orientation, remains scarce. The AI Index urges other stakeholders in the AI domain to intensify their endeavors to track diversity trends associated with AI and hopes to comprehensively cover such trends in future reports.

||Chapter Highlights|
|-|-|
|1.|U.S. and Canadian bachelor’s, master’s, and PhD CS students continue to grow more ethnically diverse. While white students continue to be the most represented ethnicity among new resident graduates at all three levels, the representation from other ethnic groups, such as Asian, Hispanic, and Black or African American students, continues to grow. For instance, since 2011, the proportion of Asian CS bachelor’s degree graduates has increased by 19.8 percentage points, and the proportion of Hispanic CS bachelor’s degree graduates has grown by 5.2 percentage points.|
|2.|Substantial gender gaps persist in European informatics, CS, CE, and IT graduates at all educational levels. Every surveyed European country reported more male than female graduates in bachelor’s, master’s, and PhD programs for informatics, CS, CE, and IT. While the gender gaps have narrowed in most countries over the last decade, the rate of this narrowing has been slow.|
|3.|U.S. K–12 CS education is growing more diverse, reflecting changes in both gender and ethnic representation. The proportion of AP CS exams taken by female students rose from 16.8% in 2007 to 30.5% in 2022. Similarly, the participation of Asian, Hispanic/Latino/Latina, and Black/African American students in AP CS has consistently increased year over year.|

### 8.1 AI Postsecondary Education

> This section examines trends in diversity within CS and AI postsecondary education across North America and Europe.

### 8.2 AI Conferences

### 8.3 K–12 Education

>This section uses data from Code.org, a U.S. nonprofit dedicated to advancing CS education in K–12 schools across the country, to paint a picture of how AI diversity trends are reflected at the K–12 level.

## Chapter 9: Public Opinion

>As AI becomes increasingly ubiquitous, it is important to understand how public perceptions regarding the technology evolve. Understanding this public opinion is vital in better anticipating AI’s societal impacts and how the integration of the technology may differ across countries and demographic groups.

> This chapter examines public opinion on AI through global, national, demographic, and ethnic perspectives. It draws upon several data sources: longitudinal survey data from Ipsos profiling global AI attitudes over time, survey data from the University of Toronto exploring public perception of ChatGPT, and data from Pew examining American attitudes regarding AI. The chapter concludes by analyzing mentions of significant AI models on Twitter, using data from Quid.

||Chapter Highlights|
|-|-|
|1.|People across the globe are more cognizant of AI’s potential impact—and more nervous. A survey from Ipsos shows that, over the last year, the proportion of those who think AI will dramatically affect their lives in the next three to five years has increased from 60% to 66%. Moreover, 52% express nervousness toward AI products and services, marking a 13 percentage point rise from 2022. In America, Pew data suggests that 52% of Americans report feeling more concerned than excited about AI, rising from 38% in 2022.|
|2.|AI sentiment in Western nations continues to be low, but is slowly improving. In 2022, several developed Western nations, including Germany, the Netherlands, Australia, Belgium, Canada, and the United States, were among the least positive about AI products and services. Since then, each of these countries has seen a rise in the proportion of respondents acknowledging the benefits of AI, with the Netherlands experiencing the most significant shift.|
|3.|The public is pessimistic about AI’s economic impact. In an Ipsos survey, only 37% of respondents feel AI will improve their job. Only 34% anticipate AI will boost the economy, and 32% believe it will enhance the job market.|
|4.|Demographic differences emerge regarding AI optimism. Significant demographic differences exist in perceptions of AI’s potential to enhance livelihoods, with younger generations generally more optimistic. For instance, 59% of Gen Z respondents believe AI will improve entertainment options, versus only 40% of baby boomers. Additionally, individuals with higher incomes and education levels are more optimistic about AI’s positive impacts on entertainment, health, and the economy than their lower-income and less-educated counterparts.|
|5.|ChatGPT is widely known and widely used. An international survey from the University of Toronto suggests that 63% of respondents are aware of ChatGPT. Of those aware, around half report using ChatGPT at least once weekly.|

### 9.1 Survey Data

* The Ipsos surveys conducted in 2022 and 2023 reveal a complex and evolving global landscape regarding public opinion on artificial intelligence (AI).
The survey consisted of interviews with 22,816 adults ages 16 to 74 in 31 countries.

|Category|Details|
|-|-|
|Global Perceptions of AI||
|Anticipated Impact|A significant 66% of respondents believe AI will greatly change their lives in the near future. This is a 6 percentage point increase from the previous year, indicating growing awareness and anticipation of AI's influence.|
|Nervousness and Trust|There's been a notable rise in nervousness about AI products and services, with 52% of respondents expressing this sentiment, up by 13 percentage points from 2022. Trust in AI companies' data-protection capabilities stands at about 50%.|
|Country-Specific Attitudes||
|Positive Views|Indonesian (78%), Thai (74%), and Mexican (73%) respondents are among the most positive about AI's benefits outweighing its drawbacks. In contrast, only 37% of Americans hold this view, reflecting significant skepticism in the U.S. and France.|
|Shifts in Perception|Countries like the Netherlands have seen a marked increase in positive views towards AI, with the percentage rising from 33% in 2022 to 43% in 2023.|
|Understanding and Excitement||
|Awareness Levels|Indonesians report high levels of understanding (84%) and excitement (75%) about AI, whereas Japanese respondents show the lowest understanding (43%) and also the least nervousness (23%).|
|Trust in Impartiality|Thai respondents exhibit the highest trust in AI's impartiality, believing it will not show bias.|
|Concerns Over AI's Impact||
|Job Impact|57% of respondents believe AI will change how they perform their current jobs within five years, and 36% fear AI may replace their jobs within the same timeframe. Younger generations, particularly Gen Z, are more likely to believe AI will affect their jobs compared to older generations.|
|Livelihoods and Quality of Life|There is a mixed perception regarding AI's impact on various aspects of life. While a majority are positive about AI improving task efficiency (54%) and entertainment (51%), fewer believe it will benefit health (39%), jobs (37%), the economy (34%), or the job market (32%).|
|ChatGPT's Influence||
|Awareness and Usage|Awareness of ChatGPT is high globally, with 63% of respondents familiar with it. The highest awareness is in India (82%), Kenya (81%), Indonesia (76%), and Pakistan (76%), while Poland reports the lowest (43%). Usage varies, with 17% using it daily and 36% weekly.|
|Concerns about AI Misuse|The top concerns globally include AI being misused for nefarious purposes (49%), its impact on jobs (49%), and potential violations of privacy (45%). Lesser concerns include issues of unequal access to AI (26%) and AI's potential for bias and discrimination (24%).|

* The Pew Research Center's investigation into American sentiment toward AI from 2021 to 2023 reveals a significant shift in public opinion. According to their latest survey data from 2023, involving 11,000 respondents, there's been a notable increase in concern about AI's role in daily life. Specifically:

|Category|Details|
|-|-|
|Increase in concern about AI||
|In 2021 and 2022|Only 37% and 38% of Americans, respectively, felt more concerned than excited about AI technology.|
|2023|This concern had risen to 52%, indicating that a majority of Americans now feel more apprehensive than enthusiastic about AI.|
|Impact in various contexts||
|Finding Products or Services Online|49% of respondents believe AI is beneficial in helping people find products or services.|
|Safeguarding Personal Information Privacy|53% of respondents feel AI is more likely to be detrimental than beneficial in protecting personal information privacy.|
|Higher education and more positive views||
|Healthcare|Individuals with college or higher-level degrees are more likely to believe AI can significantly aid doctors in delivering quality patient care.|
|Product and Service Discovery|Those with higher education levels are also more optimistic about AI's ability to assist people in discovering interesting products and services online.|

### 9.2 Social Media Data

* The analysis of public attitudes toward AI models through social media posts provides insightful data on how these technologies are perceived. This can be broken down into two main metrics: net sentiment score and proportion of social media conversation.

|Category|Details|
|-|-|
|Net Sentiment Score||
|GraphCast|A new AI-powered weather forecasting system from DeepMind, received highly positive sentiment.|
|Claude 2.1|One of Anthropic’s latest language models, also received a significant amount of positive sentiment.|
|General Trend|Many AI models released in 2023 were met with positive social media sentiment, indicating general optimism and approval from the public.|
|Proportion of Social Media Conversation||
|GPT-4|Despite many new models being released in 2023, GPT-4 was the most talked-about model, capturing 45% of social media conversation throughout the year.|
|Other Models|Grok, Stable Diffusion, and Gemini were also significant topics of discussion, though they did not match the dominance of GPT-4.|
|Interpretation||
|GPT-4 Dominance|The high proportion of attention on GPT-4 suggests it remains a benchmark in AI discussions, possibly due to its wide range of applications, continuous improvements, or strong brand recognition from OpenAI.|
|Positive Reception of New Models|The positive net sentiment scores for new models like GraphCast and Claude 2.1 reflect a public eagerness to embrace new and diverse AI technologies, especially those offering novel applications such as weather forecasting and advanced language processing.|
|Sustained Interest|The sustained interest in GPT-4 despite new releases highlights the model's significance and the challenge new entrants face in capturing the same level of public attention.|

* In 2023, the social media discussion around AI focused primarily on GPT-4, Stable Diffusion, and newly launched AI models like Google Gemini and xAI's Grok.

|Topic|Details|
|-|-|
|GPT-4||
|Q2 2023|The launch of GPT-4 on March 14, 2023, generated significant discussion. Positive sentiment was driven by improvements such as faster processing speeds, better accuracy, and enhanced productivity for tasks like coding, corporate collaboration, and content creation. Negative sentiment emerged from complaints about occasional website crashes and an open letter led by Elon Musk, signed by over 1,300 AI experts, urging a pause in training powerful AI systems due to potential risks.|
|Q4 2023|The release of GPT-4 Turbo in November brought another surge in discussions. Users praised its longer conversation capabilities, improved contextual understanding, and multimodal abilities to generate images. However, some disappointment was expressed regarding its knowledge cutoff in April 2023 and slower loading speeds compared to the standard GPT-4. Sample posts highlighted the transformative impact on programming workflows and practical applications like enhancing LinkedIn profiles.|
|Stable Diffusion|Discussions about Stable Diffusion were more prominent in the first half of 2023,particularly focusing on the Stable Diffusion XL models, which were mentioned significantly more than Stable Diffusion 2.0. Positive sentiment was driven by the tool’s growing popularity, its potential to boost creativity, and its technical advancements like enhanced accuracy and higher resolution. Negative sentiment centered around legal and ethical issues related to AI-generated content, including copyright violations, ownership of AI-created material, and fears of AI replacing human artists. There were also concerns about the risks of AI spreading misinformation and facilitating academic cheating. Posts highlighted the excitement over technical achievements, such as significant speedups and energy savings from model compression.|
|Gemini and Grok|Both Google’s Gemini and xAI’s Grok saw increased conversation in Q4 2023 following their late-year launches. Positive feedback for Gemini included its improved accuracy, multilingual capabilities, and potential to enhance Google services like Search and Ads. Negative feedback was related to inaccurate results, disappointment over delayed release, and skepticism about the AI demo's claims. Grok discussions were less detailed but similarly reflected a mix of anticipation and scrutiny.|
