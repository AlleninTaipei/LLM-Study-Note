# LLM-Study-Note

|Topics|Description|
|-|-|
|[LLM Deploement - llama.cpp](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/LLM%20Deployment%20-%20llama.cpp.md)|The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.|
|[Common LLM-Related Files](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Common%20LLM-Related%20Files.md)|Understanding these files is critical for anyone working with LLMs, especially if you're doing local inference, fine-tuning, deployment, or managing LLMOps pipelines.|
|[A Comprehensive Overview from Training to Inference](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/A%20Comprehensive%20Overview%20from%20Training%20to%20Inference.md)|The primary objective of this paper is to provide a comprehensive overview of LLMs training and inference techniques to equip researchers with the knowledge required for developing, deploying, and applying LLMs.|
|[A Developer’s Guide To LLMOps](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/A%20Developer%E2%80%99s%20Guide%20To%20LLMOps.md)|As teams deploy large language models to production, the same challenges around performance and task measurement still exist. Hence, LLMOps is essential to scale large language models and deploy them to production effectively.|
|[How to Productionize Large Language Models](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/How%20to%20Productionize%20Large%20Language%20Models.md)|Understand LLMOps, architectural patterns, how to evaluate, fine tune & deploy HuggingFace generative AI models locally or on cloud.|
|[Generate and Use Synthetic Data for Finetuning](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Generate%20and%20Use%20Synthetic%20Data%20for%20Finetuning.md)|Synthetic data is generally defined as artificially annotated information generated by computer algorithms or simulations.|
|[Multi GPUs Training LLMs From Scratch](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Multi-GPU_Training_LLMs_From_Scratch.md)|Explained by [Hsiu-Hsuan Wang](https://anthony-wss.github.io/), a graduate student in the Department of Electrical Engineering and Computer Science at National Taiwan University, published on [Professor Hung-yi Lee’s Youtube channel](https://www.youtube.com/@HungyiLeeNTU).|
|[MCP - Model Context Protocol](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/MCP%20-%20Model%20Context%20Protocol.md)|"LLMs by themselves are incapable of doing anything meaningful... The only thing an LLM in its current state is good at is predicting the next text." - Ross Mike<br>"Think of every tool that I have to connect to make my LLM valuable as a different language... MCP, you can consider it to be a layer between your LLM and the services and the tools." - Ross Mike|

## Valuable links

The following are highly valuable links that are worth exploring in depth.

### [LLM course](https://github.com/mlabonne/llm-course)

The LLM course is divided into three parts.
* LLM Fundamentals covers essential knowledge about mathematics, Python, and neural networks.
* The LLM Scientist focuses on building the best possible LLMs using the latest techniques.
* The LLM Engineer focuses on creating LLM-based applications and deploying them.

### [Intro to Large Language Models - Andrej Karpathy](https://youtu.be/zjkBMFhNj_g?si=4sWGbJP9kFxDMbX9)

... in this stage you write out some labeling instructions that basically specify how your assistant should behave then you hire people so for example scale AI is a company that actually would work with you to actually basically create documents according to your labeling instructions
**you collect 100,000 as an example high quality ideal Q&A responses**
and then you would fine-tune the base model on this data this is a lot cheaper this would only potentially take like one day or something like that instead of a few months or something ...

### [Meta's Roadmap for Full Stack AI: Insights from Joe Spisak | Ray Summit 2024](https://youtu.be/QS7C3ZCI8Dw?si=hOC66HkbH7kvDIys)

... the 1B and 3B respectively in post training this is actually where like things got really interesting right **we generated synthetic data from the 405b that was really useful we then trained in sft on synthetic data for both the 1B and 3B and that's where like having really high quality data from a foundation model is incredibly powerful** and that the results really show ... sft = Supervised Fine-Tuning

### [Reinforcement Fine-Tuning—12 Days of OpenAI: Day 2](https://youtu.be/yCIYS9fx56U?si=_sOHvpRC_zMfJl_n)

**Introduced a method to fine-tune 01 models using reinforcement learning for specific domains (e.g., law, finance, medicine). Demonstrated with rare disease research. Fine-tuning enables specialized AI models with minimal data (few dozen examples).**

... what you're teaching it to do is to learn to reason in entirely new ways over custom domains and the way this works is that we when the model sees a problem we give it space to Think Through the problem and then we grade the final answer from the model and then using the power of reinforcement learning we reinforce lines of thinking that led to correct answers and we disincentivize lines of thinking that led to incorrect answers and what you'll see is that you know with as little as a few dozen examples the model will learn to reason in new and effective ways over custom domains **that's crazy that you can do that with just 12 examples ...**

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Made with ❤️ by [Allen Sun](https://github.com/allenintaipei)
