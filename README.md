# LLM-Study-Note

|Topics|Description|
|-|-|
|[AI Introduction](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/AI%20Introduction.md)|Understand the basics of Artificial Intelligence including the key concepts and terminology.|
|[Artificial Intelligence Index Report 2024](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Artificial%20Intelligence%20Index%20Report%202024.md)|The 2024 Index is most comprehensive to date and arrives at an important moment when AI’s influence on society has never been more pronounced.<br>Added the Artificial Intelligence Index Report 2024 to the repository. Provide a summary of the key points from this report.|
|[Promprt Engineering](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Prompt%20Engineering.md)|The president of NVIDIA, Jensen Huang said during an interview with the UAE’s Minister of AI, Omar al-Olama at the World Government Summit in Dubai.<br>***Everybody in the world is now a programmer. This is the miracle of AI. For the very first time, the technology divide has been completely closed. To engage with AI is a lot easier now than at any time in the history of computing.***|
|[LLM: Parameters and Memory Estimation](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/LLM%20Parameters%20and%20Memory%20Estimation.md)|Understanding the memory requirements of models is vital for optimizing deployment and usage. It impacts the choice of hardware and the overall cost.<br>Discover a practical estimate for the VRAM needs of a system, based on a general understanding, along with viable solutions available on the internet.<br>By framing the information in terms of scaling laws, it becomes clear how each factor contributes to the overall performance and what trade-offs must be considered when scaling up LLMs.|
|[Understanding Tokens per second in NLP](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Understanding%20Tokens%20per%20second%20in%20NLP.md)|This comprehensive coverage provides a solid foundation for understanding, measuring, and optimizing tokens per second in machine learning models.|
|[SLMs - Small Language Models ](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/SLMs%20-%20Small%20Language%20Models%20.md)|***Microsoft Phi-3 small model is inspired by "daughter's bedtime story"***<br>Windows Copilot Runtime introduces new ways to interact with operating systems that leverage AI, such as the Small Language Model (SLM) built by Microsoft Research, which provides many of the same capabilities found in large language models (LLM), but is more streamlined and More efficient so that it can be executed natively on Windows.|
|[Windows AI PC](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Windows%20AI%20PC.md)|Microsoft wants to bring generative AI to the forefront of Windows — and the PCs running it. Provide a simple table to explain the general concept of Copilot+ PCs.<br>The other Windows AI PC examples are RTX AI PC and Gigabyte AI TOP.|
|[Distinction between NPUs and GPUs](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Distinction%20between%20NPUs%20and%20GPUs.md#distinction-between-npus-and-gpus)|The features of NPU design enable it to achieve higher energy efficiency compared to traditional GPUs when handling AI computations.<br>This means that for the same computational workload, the energy required by an NPU will be significantly lower.<br>For businesses, this not only translates to cost savings but also aligns with global environmental sustainability trends.|
|[Comprehensive Comparison of Edge AI Processors](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Comprehensive%20Comparison%20of%20Edge%20AI%20Processors.md)|In this article, we provide an in-depth comparison of AI processors designed for edge computing from leading companies including NVIDIA, Intel, Google, Qualcomm, Ambarella, Xilinx, Arm, Mythic, NXP, and DEEPX.<br>We cover their specifications, target applications, software support, and key strengths, along with a brief market share overview for each. This comparison helps to understand the competitive landscape and the unique offerings of each player in the edge AI processor market.|
|[AI Engineer - Workflow and Necessary Skills](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/AI%20Engineer%20-%20%20Workflow%20and%20Necessary%20Skills.md)|What exactly does an AI engineer do ?<br>What abilities do they need ?<br>Are there any requirements for their major ?<br>The head of Elan's artificial intelligence R&D department will answer the questions.|
|[Introduction to Data Engineers](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Introduction%20to%20Data%20Engineers.md)|What is a data engineer ?<br>The daily work of a data scientist.|
|[LLM Deploement - llama.cpp](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/LLM%20Deployment%20-%20llama.cpp.md)|The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.|
|[Common LLM-Related Files](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Common%20LLM-Related%20Files.md)|Understanding these files is critical for anyone working with LLMs, especially if you're doing local inference, fine-tuning, deployment, or managing LLMOps pipelines.|
|[A Comprehensive Overview from Training to Inference](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/A%20Comprehensive%20Overview%20from%20Training%20to%20Inference.md)|The primary objective of this paper is to provide a comprehensive overview of LLMs training and inference techniques to equip researchers with the knowledge required for developing, deploying, and applying LLMs.|
|[A Developer’s Guide To LLMOps](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/A%20Developer%E2%80%99s%20Guide%20To%20LLMOps.md)|As teams deploy large language models to production, the same challenges around performance and task measurement still exist. Hence, LLMOps is essential to scale large language models and deploy them to production effectively.|
|[How to Productionize Large Language Models](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/How%20to%20Productionize%20Large%20Language%20Models.md)|Understand LLMOps, architectural patterns, how to evaluate, fine tune & deploy HuggingFace generative AI models locally or on cloud.|
|[Generate and Use Synthetic Data for Finetuning](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Generate%20and%20Use%20Synthetic%20Data%20for%20Finetuning.md)|Synthetic data is generally defined as artificially annotated information generated by computer algorithms or simulations.|
|[Multi GPUs Training LLMs From Scratch](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/Multi-GPU_Training_LLMs_From_Scratch.md)|Explained by [Hsiu-Hsuan Wang](https://anthony-wss.github.io/), a graduate student in the Department of Electrical Engineering and Computer Science at National Taiwan University, published on [Professor Hung-yi Lee’s Youtube channel](https://www.youtube.com/@HungyiLeeNTU).|
|[MCP - Model Context Protocol](https://github.com/AlleninTaipei/LLM-Study-Note/blob/main/MCP%20-%20Model%20Context%20Protocol.md)|"LLMs by themselves are incapable of doing anything meaningful... The only thing an LLM in its current state is good at is predicting the next text." - Ross Mike<br>"Think of every tool that I have to connect to make my LLM valuable as a different language... MCP, you can consider it to be a layer between your LLM and the services and the tools." - Ross Mike|

## Valuable links

The following are highly valuable links that are worth exploring in depth.

### [LLM course](https://github.com/mlabonne/llm-course)

The LLM course is divided into three parts.
* LLM Fundamentals covers essential knowledge about mathematics, Python, and neural networks.
* The LLM Scientist focuses on building the best possible LLMs using the latest techniques.
* The LLM Engineer focuses on creating LLM-based applications and deploying them.

### [Intro to Large Language Models - Andrej Karpathy](https://youtu.be/zjkBMFhNj_g?si=4sWGbJP9kFxDMbX9)

... in this stage you write out some labeling instructions that basically specify how your assistant should behave then you hire people so for example scale AI is a company that actually would work with you to actually basically create documents according to your labeling instructions
**you collect 100,000 as an example high quality ideal Q&A responses**
and then you would fine-tune the base model on this data this is a lot cheaper this would only potentially take like one day or something like that instead of a few months or something ...

### [Meta's Roadmap for Full Stack AI: Insights from Joe Spisak | Ray Summit 2024](https://youtu.be/QS7C3ZCI8Dw?si=hOC66HkbH7kvDIys)

... the 1B and 3B respectively in post training this is actually where like things got really interesting right **we generated synthetic data from the 405b that was really useful we then trained in sft on synthetic data for both the 1B and 3B and that's where like having really high quality data from a foundation model is incredibly powerful** and that the results really show ... sft = Supervised Fine-Tuning

### [Reinforcement Fine-Tuning—12 Days of OpenAI: Day 2](https://youtu.be/yCIYS9fx56U?si=_sOHvpRC_zMfJl_n)

**Introduced a method to fine-tune 01 models using reinforcement learning for specific domains (e.g., law, finance, medicine). Demonstrated with rare disease research. Fine-tuning enables specialized AI models with minimal data (few dozen examples).**

... what you're teaching it to do is to learn to reason in entirely new ways over custom domains and the way this works is that we when the model sees a problem we give it space to Think Through the problem and then we grade the final answer from the model and then using the power of reinforcement learning we reinforce lines of thinking that led to correct answers and we disincentivize lines of thinking that led to incorrect answers and what you'll see is that you know with as little as a few dozen examples the model will learn to reason in new and effective ways over custom domains **that's crazy that you can do that with just 12 examples ...**

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Made with ❤️ by [Allen Sun](https://github.com/allenintaipei)
