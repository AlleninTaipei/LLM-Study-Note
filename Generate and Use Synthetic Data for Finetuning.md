# Generate and Use Synthetic Data for Finetuning

[Source: Machine Learning for Synthetic Data Generation: A Review](https://arxiv.org/html/2302.04062v9#S1) **Synthetic data is generally defined as artificially annotated information generated by computer algorithms or simulations.**

[Google denies it copied ChatGPT to train its own AI chatbot Bard](https://economictimes.indiatimes.com/tech/technology/google-denies-it-copied-chatgpt-to-train-its-own-ai-chatbot-bard/articleshow/99107700.cms?utm_source=contentofinterest&utm_medium=text&utm_campaign=cppst)

[OpenAI suspends ByteDance’s account over ‘secretly using’ its AI tech](https://economictimes.indiatimes.com/tech/technology/openai-suspends-bytedances-account-over-secretly-using-its-ai-tech/articleshow/106042266.cms?utm_source=contentofinterest&utm_medium=text&utm_campaign=cppst)

[Introducing Llama 3.1: Our most capable models to date](https://ai.meta.com/blog/meta-llama-3-1/)

## Size of a dataset for fine-tuning a large language model (LLM)

When deciding the size of a dataset for fine-tuning a large language model (LLM), the major factors to consider are:

|Major factors|Consideration|
|-|-|
|Task Complexity|The complexity of the task or domain you are fine-tuning for affects the dataset size. More complex tasks or those requiring specialized knowledge typically need larger datasets to capture the required nuances.|
|Model Size|Larger models generally need more data to fine-tune effectively because they have more parameters to adjust. A smaller dataset may lead to underfitting, where the model doesn't learn enough to perform well on the task.|
|Overfitting Risk|Smaller datasets increase the risk of overfitting, where the model learns too much from the fine-tuning data and performs poorly on general or unseen examples. A larger dataset can help reduce this risk.|
|Training Resources|Larger datasets require more computational resources (time, memory, processing power) for fine-tuning. If resources are limited, it may be necessary to use a smaller dataset or optimize for more efficient methods like PEFT (e.g., LoRA).|
|Domain-Specific Requirements|For specialized tasks, even a small dataset may be effective if the task requires highly specific knowledge (e.g., legal documents, medical records). The dataset size should balance specificity and generalizability.|
|Diminishing Returns|At a certain point, increasing dataset size may yield diminishing returns, where additional data does not significantly improve performance. The size should be chosen to cover a wide enough variety without being excessively large.|

## General understanding in Domain-specific requirements

Example: Given the nature of the task, the model should handle a variety of customer queries related to motherboard specifications, troubleshooting, warranty, installation, etc. Here's a general guideline on dataset size for both 3B and 7B pre-trained models:

  * 3B Model (Smaller Model) Dataset Size: ~10K to 50K examples
    Reasoning: A 3B parameter model can perform well with a moderate dataset size, but its smaller capacity means it may need more focused data. Given the domain-specific nature (motherboard product queries), starting with 10K-50K carefully curated examples (covering common user issues, troubleshooting steps, and FAQs) would be sufficient to enable good generalization without overloading the model.
    Focus: Ensure that the dataset covers a wide range of motherboard-related questions, common problems, and specific answers related to product features.
  * 7B Model (Larger Model) Dataset Size: ~50K to 150K+ examples
    Reasoning: A 7B model has greater capacity and can benefit from a larger dataset to fully capture the nuances and details of product support queries. You might aim for 50K-150K+ examples, especially if the chatbot will handle diverse customer queries or more complex scenarios like in-depth technical troubleshooting or product comparison.
    Focus: A larger dataset should include more variety in phrasing, complex problem scenarios, and detailed explanations. You may also consider including multilingual support if the chatbot will cater to different regions.

## The process of collecting and annotating data

The process of collecting and annotating data is both time-consuming and expensive, leading to several challenges. Since machine learning is highly dependent on data, some of the key hurdles include:

  * Data quality: Ensuring high data quality is one of the most significant challenges faced by machine learning professionals. Poor-quality data can lead to incorrect or imprecise model predictions, causing confusion and misinterpretation.
  * Data scarcity: A substantial part of the current AI challenge arises from limited data availability. Either there are not enough datasets, or the cost of manual labeling is prohibitively high.
  * Data privacy and fairness: In many cases, datasets cannot be publicly shared due to privacy and fairness concerns. In such situations, generating synthetic data can be a viable solution.

## Self-Instruct

[Self-Instruct](https://arxiv.org/abs/2212.10560) improves the instruction-following ability of a non-finetuned model (vanilla gpt-3) by bootstrapping off its own generations. First, they generate instructions, input context, and responses from the model. Then, they filter invalid or similar examples before using the remaining samples to finetune the original model. 

[How Self-Instruct works?](https://github.com/yizhongw/self-instruct#how-self-instruct-works)

## Distillation

Knowledge distillation is a technique that facilitates the transfer of knowledge from large, computationally expensive models to smaller, more efficient ones, while preserving accuracy. This enables deployment on less powerful hardware, resulting in faster and more efficient model evaluation.

* [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) finetuned llama-7b on 52k instruction-following samples generated from gpt-3.5 (text-davinci-003).
* [Vicuna](https://arxiv.org/abs/2306.05685)
* [WizardLM](https://arxiv.org/abs/2304.12244)
* [Orca](https://arxiv.org/abs/2306.02707)
* [Orca 2](https://arxiv.org/abs/2311.11045)
* [WizardCoder](https://arxiv.org/abs/2306.08568)
* [Magicoder](https://arxiv.org/abs/2312.02120)
* [WaveCoder](https://arxiv.org/abs/2312.14187)
* [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)
* [Textbooks Are All You Need II](https://arxiv.org/abs/2309.05463)
* [Starling-7B](https://starling.cs.berkeley.edu/)

## Self-improvement

* Self-improvement approaches don’t rely on an external model to generate synthetic data. Instead, they bootstrap on the model that’s being finetuned over several iterations of generation, self-evaluation, and finetuning.

### Synthetic data for instruction-tuning

* [Self-Alignment with Instruction Backtranslation](https://arxiv.org/abs/2308.06259)
* [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/abs/2401.01335)
* [Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://arxiv.org/abs/2312.06585)

### Synthetic data for preference-tuning

* [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)
* [SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF](https://arxiv.org/abs/2310.05344)
* [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)

### Synthetic data for pretraining

* [Solving olympiad geometry without human demonstrations](https://www.nature.com/articles/s41586-023-06747-5)
* [Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling](https://arxiv.org/abs/2401.16380)

